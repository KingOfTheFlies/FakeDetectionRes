npr_full_train.py
Model is running on device: cuda:0
cwd: /home/jovyan/shares/SR006.nfs2/almas_deepfake_detection/deepfake_detection_models/NPR-DeepfakeDetection
2024_08_10_08_05_27 Train loss: 0.30224329233169556 at step: 400 lr 0.0002
2024_08_10_08_06_09 Train loss: 0.09340367466211319 at step: 800 lr 0.0002
2024_08_10_08_06_51 Train loss: 0.04434806853532791 at step: 1200 lr 0.0002
2024_08_10_08_07_32 Train loss: 0.05624959617853165 at step: 1600 lr 0.0002
2024_08_10_08_08_14 Train loss: 0.006928067188709974 at step: 2000 lr 0.0002
2024_08_10_08_08_55 Train loss: 0.021086201071739197 at step: 2400 lr 0.0002
2024_08_10_08_09_37 Train loss: 0.004110141657292843 at step: 2800 lr 0.0002
2024_08_10_08_10_19 Train loss: 0.010559131391346455 at step: 3200 lr 0.0002
2024_08_10_08_11_00 Train loss: 0.00680173933506012 at step: 3600 lr 0.0002
2024_08_10_08_11_42 Train loss: 0.0013755103573203087 at step: 4000 lr 0.0002
2024_08_10_08_12_23 Train loss: 0.0024561837781220675 at step: 4400 lr 0.0002
saving the model at the end of epoch 0, iters 4501
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_latest.pth
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_0.pth
(Val @ epoch 0) acc: 0.985125; ap: 0.9988346584096734
*************************
2024_08_10_08_13_00
(0 progan    ) acc: 99.0; ap: 99.9
(1 stylegan  ) acc: 89.0; ap: 96.0
(2 stylegan2 ) acc: 93.4; ap: 98.6
(3 biggan    ) acc: 66.1; ap: 79.3
(4 cyclegan  ) acc: 63.3; ap: 84.5
(5 stargan   ) acc: 92.1; ap: 99.7
(6 gaugan    ) acc: 52.6; ap: 56.0
(7 deepfake  ) acc: 66.4; ap: 62.8
(8 Mean      ) acc: 77.7; ap: 84.6
*************************
2024_08_10_08_17_06
2024_08_10_08_17_42 Train loss: 0.08761237561702728 at step: 4800 lr 0.0002
2024_08_10_08_18_24 Train loss: 0.0031098758336156607 at step: 5200 lr 0.0002
2024_08_10_08_19_05 Train loss: 0.0001297439739573747 at step: 5600 lr 0.0002
2024_08_10_08_19_47 Train loss: 0.11321797966957092 at step: 6000 lr 0.0002
2024_08_10_08_20_29 Train loss: 0.0009091660031117499 at step: 6400 lr 0.0002
2024_08_10_08_21_10 Train loss: 0.0003436714760027826 at step: 6800 lr 0.0002
2024_08_10_08_21_52 Train loss: 0.0428062379360199 at step: 7200 lr 0.0002
2024_08_10_08_22_33 Train loss: 0.0009332221234217286 at step: 7600 lr 0.0002
2024_08_10_08_23_15 Train loss: 0.006919833831489086 at step: 8000 lr 0.0002
2024_08_10_08_23_56 Train loss: 0.0010473596630617976 at step: 8400 lr 0.0002
2024_08_10_08_24_38 Train loss: 0.0206867977976799 at step: 8800 lr 0.0002
(Val @ epoch 1) acc: 0.99425; ap: 0.9998868078131729
*************************
2024_08_10_08_25_24
(0 progan    ) acc: 99.6; ap: 100.0
(1 stylegan  ) acc: 89.8; ap: 97.0
(2 stylegan2 ) acc: 96.4; ap: 99.7
(3 biggan    ) acc: 73.2; ap: 81.9
(4 cyclegan  ) acc: 76.5; ap: 96.2
(5 stargan   ) acc: 85.7; ap: 100.0
(6 gaugan    ) acc: 51.6; ap: 55.5
(7 deepfake  ) acc: 71.9; ap: 84.4
(8 Mean      ) acc: 80.6; ap: 89.3
*************************
2024_08_10_08_29_30
2024_08_10_08_29_55 Train loss: 0.006106582470238209 at step: 9200 lr 0.0002
2024_08_10_08_30_36 Train loss: 0.007454189471900463 at step: 9600 lr 0.0002
2024_08_10_08_31_18 Train loss: 0.007025695871561766 at step: 10000 lr 0.0002
2024_08_10_08_32_00 Train loss: 0.004571566823869944 at step: 10400 lr 0.0002
2024_08_10_08_32_41 Train loss: 0.0028637894429266453 at step: 10800 lr 0.0002
2024_08_10_08_33_23 Train loss: 0.012740216217935085 at step: 11200 lr 0.0002
2024_08_10_08_34_04 Train loss: 0.034727469086647034 at step: 11600 lr 0.0002
2024_08_10_08_34_46 Train loss: 0.006312781013548374 at step: 12000 lr 0.0002
2024_08_10_08_35_28 Train loss: 0.0012418559053912759 at step: 12400 lr 0.0002
2024_08_10_08_36_09 Train loss: 0.0031764896120876074 at step: 12800 lr 0.0002
2024_08_10_08_36_51 Train loss: 0.0022618467919528484 at step: 13200 lr 0.0002
(Val @ epoch 2) acc: 0.995375; ap: 0.9998561792181937
*************************
2024_08_10_08_37_48
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 90.0; ap: 97.9
(2 stylegan2 ) acc: 93.9; ap: 99.7
(3 biggan    ) acc: 79.0; ap: 86.5
(4 cyclegan  ) acc: 87.5; ap: 98.3
(5 stargan   ) acc: 99.9; ap: 100.0
(6 gaugan    ) acc: 70.9; ap: 71.1
(7 deepfake  ) acc: 65.6; ap: 70.8
(8 Mean      ) acc: 85.8; ap: 90.6
*************************
2024_08_10_08_41_55
2024_08_10_08_42_10 Train loss: 0.002261088229715824 at step: 13600 lr 0.0002
2024_08_10_08_42_52 Train loss: 0.001365648116916418 at step: 14000 lr 0.0002
2024_08_10_08_43_34 Train loss: 6.576738087460399e-05 at step: 14400 lr 0.0002
2024_08_10_08_44_15 Train loss: 0.0023306237999349833 at step: 14800 lr 0.0002
2024_08_10_08_44_57 Train loss: 0.0025300283450633287 at step: 15200 lr 0.0002
2024_08_10_08_45_38 Train loss: 0.00011817057384178042 at step: 15600 lr 0.0002
2024_08_10_08_46_20 Train loss: 0.003661409020423889 at step: 16000 lr 0.0002
2024_08_10_08_47_02 Train loss: 0.0001089144207071513 at step: 16400 lr 0.0002
2024_08_10_08_47_43 Train loss: 6.6154661908512935e-06 at step: 16800 lr 0.0002
2024_08_10_08_48_25 Train loss: 2.6434008759679273e-05 at step: 17200 lr 0.0002
2024_08_10_08_49_07 Train loss: 0.00024524208856746554 at step: 17600 lr 0.0002
2024_08_10_08_49_49 Train loss: 0.0003364423173479736 at step: 18000 lr 0.0002
(Val @ epoch 3) acc: 0.9925; ap: 0.9997228651698393
*************************
2024_08_10_08_50_15
(0 progan    ) acc: 99.4; ap: 100.0
(1 stylegan  ) acc: 91.0; ap: 98.0
(2 stylegan2 ) acc: 98.3; ap: 99.9
(3 biggan    ) acc: 74.6; ap: 82.7
(4 cyclegan  ) acc: 78.9; ap: 97.7
(5 stargan   ) acc: 94.8; ap: 100.0
(6 gaugan    ) acc: 60.7; ap: 66.5
(7 deepfake  ) acc: 67.9; ap: 74.0
(8 Mean      ) acc: 83.2; ap: 89.9
*************************
2024_08_10_08_54_18
2024_08_10_08_55_04 Train loss: 0.00022183255350682884 at step: 18400 lr 0.0002
2024_08_10_08_55_46 Train loss: 0.0027793238405138254 at step: 18800 lr 0.0002
2024_08_10_08_56_28 Train loss: 0.0949121043086052 at step: 19200 lr 0.0002
2024_08_10_08_57_09 Train loss: 8.975684613687918e-05 at step: 19600 lr 0.0002
2024_08_10_08_57_51 Train loss: 0.0001571816683281213 at step: 20000 lr 0.0002
2024_08_10_08_58_32 Train loss: 5.415283885668032e-05 at step: 20400 lr 0.0002
2024_08_10_08_59_14 Train loss: 0.00015393593639601022 at step: 20800 lr 0.0002
2024_08_10_08_59_55 Train loss: 0.0001333382388111204 at step: 21200 lr 0.0002
2024_08_10_09_00_37 Train loss: 0.011738795787096024 at step: 21600 lr 0.0002
2024_08_10_09_01_19 Train loss: 0.00022487813839688897 at step: 22000 lr 0.0002
2024_08_10_09_02_00 Train loss: 1.6180427337531e-05 at step: 22400 lr 0.0002
(Val @ epoch 4) acc: 0.99325; ap: 0.9997456781858234
*************************
2024_08_10_09_02_38
(0 progan    ) acc: 99.5; ap: 100.0
(1 stylegan  ) acc: 91.3; ap: 97.7
(2 stylegan2 ) acc: 93.6; ap: 99.4
(3 biggan    ) acc: 78.2; ap: 84.1
(4 cyclegan  ) acc: 87.9; ap: 98.9
(5 stargan   ) acc: 95.3; ap: 100.0
(6 gaugan    ) acc: 60.6; ap: 61.2
(7 deepfake  ) acc: 65.6; ap: 71.5
(8 Mean      ) acc: 84.0; ap: 89.1
*************************
2024_08_10_09_06_40
2024_08_10_09_07_16 Train loss: 3.856596595142037e-05 at step: 22800 lr 0.0002
2024_08_10_09_07_58 Train loss: 0.0004357228463049978 at step: 23200 lr 0.0002
2024_08_10_09_08_39 Train loss: 0.0008016039500944316 at step: 23600 lr 0.0002
2024_08_10_09_09_21 Train loss: 4.9898641009349376e-05 at step: 24000 lr 0.0002
2024_08_10_09_10_03 Train loss: 8.405721018789336e-06 at step: 24400 lr 0.0002
2024_08_10_09_10_44 Train loss: 0.00032871708390302956 at step: 24800 lr 0.0002
2024_08_10_09_11_26 Train loss: 0.03504404425621033 at step: 25200 lr 0.0002
2024_08_10_09_12_07 Train loss: 1.0940034371742513e-05 at step: 25600 lr 0.0002
2024_08_10_09_12_49 Train loss: 0.006163906771689653 at step: 26000 lr 0.0002
2024_08_10_09_13_30 Train loss: 0.00013927930558566004 at step: 26400 lr 0.0002
2024_08_10_09_14_12 Train loss: 0.0010015929583460093 at step: 26800 lr 0.0002
(Val @ epoch 5) acc: 0.9965; ap: 0.9999412554398204
*************************
2024_08_10_09_15_00
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 90.6; ap: 98.4
(2 stylegan2 ) acc: 93.2; ap: 99.6
(3 biggan    ) acc: 82.6; ap: 89.2
(4 cyclegan  ) acc: 88.8; ap: 97.9
(5 stargan   ) acc: 99.3; ap: 100.0
(6 gaugan    ) acc: 78.7; ap: 79.0
(7 deepfake  ) acc: 55.8; ap: 57.5
(8 Mean      ) acc: 86.1; ap: 90.2
*************************
2024_08_10_09_19_05
2024_08_10_09_19_31 Train loss: 6.234314059838653e-05 at step: 27200 lr 0.0002
2024_08_10_09_20_12 Train loss: 0.00017094664508476853 at step: 27600 lr 0.0002
2024_08_10_09_21_03 Train loss: 3.2265481422655284e-05 at step: 28000 lr 0.0002
2024_08_10_09_21_52 Train loss: 0.00017640739679336548 at step: 28400 lr 0.0002
2024_08_10_09_22_39 Train loss: 0.0006698144134134054 at step: 28800 lr 0.0002
2024_08_10_09_23_28 Train loss: 0.007593024522066116 at step: 29200 lr 0.0002
2024_08_10_09_24_12 Train loss: 0.0005077373934909701 at step: 29600 lr 0.0002
2024_08_10_09_24_54 Train loss: 6.169931293698028e-05 at step: 30000 lr 0.0002
2024_08_10_09_25_36 Train loss: 6.170020060380921e-05 at step: 30400 lr 0.0002
2024_08_10_09_26_18 Train loss: 0.0006852729711681604 at step: 30800 lr 0.0002
2024_08_10_09_26_59 Train loss: 3.7877485738135874e-05 at step: 31200 lr 0.0002
(Val @ epoch 6) acc: 0.994625; ap: 0.9997158584503217
*************************
2024_08_10_09_27_58
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 90.6; ap: 98.1
(2 stylegan2 ) acc: 94.5; ap: 99.8
(3 biggan    ) acc: 75.2; ap: 85.9
(4 cyclegan  ) acc: 67.3; ap: 97.0
(5 stargan   ) acc: 91.3; ap: 100.0
(6 gaugan    ) acc: 60.6; ap: 69.1
(7 deepfake  ) acc: 59.9; ap: 66.6
(8 Mean      ) acc: 79.9; ap: 89.6
*************************
2024_08_10_09_32_12
2024_08_10_09_32_28 Train loss: 0.0007264605374075472 at step: 31600 lr 0.0002
2024_08_10_09_33_09 Train loss: 0.00032024012762121856 at step: 32000 lr 0.0002
2024_08_10_09_33_51 Train loss: 0.16993612051010132 at step: 32400 lr 0.0002
2024_08_10_09_34_33 Train loss: 0.0016360377194359899 at step: 32800 lr 0.0002
2024_08_10_09_35_14 Train loss: 0.00021823737188242376 at step: 33200 lr 0.0002
2024_08_10_09_35_56 Train loss: 8.919856190914288e-05 at step: 33600 lr 0.0002
2024_08_10_09_36_37 Train loss: 0.00013838151062373072 at step: 34000 lr 0.0002
2024_08_10_09_37_19 Train loss: 0.00022632500622421503 at step: 34400 lr 0.0002
2024_08_10_09_38_01 Train loss: 0.0010389944072812796 at step: 34800 lr 0.0002
2024_08_10_09_38_45 Train loss: 7.661961717531085e-05 at step: 35200 lr 0.0002
2024_08_10_09_39_44 Train loss: 7.628716412000358e-05 at step: 35600 lr 0.0002
2024_08_10_09_41_30 Train loss: 0.0001796931028366089 at step: 36000 lr 0.0002
(Val @ epoch 7) acc: 0.98475; ap: 0.9996171660441867
*************************
2024_08_10_09_42_15
(0 progan    ) acc: 98.9; ap: 100.0
(1 stylegan  ) acc: 88.6; ap: 98.9
(2 stylegan2 ) acc: 89.7; ap: 99.8
(3 biggan    ) acc: 77.5; ap: 81.9
(4 cyclegan  ) acc: 94.4; ap: 99.2
(5 stargan   ) acc: 97.1; ap: 100.0
(6 gaugan    ) acc: 67.7; ap: 65.0
(7 deepfake  ) acc: 62.2; ap: 61.9
(8 Mean      ) acc: 84.5; ap: 88.3
*************************
2024_08_10_09_46_18
2024_08_10_09_47_04 Train loss: 6.598941581614781e-06 at step: 36400 lr 0.0002
2024_08_10_09_47_45 Train loss: 0.002703206380829215 at step: 36800 lr 0.0002
2024_08_10_09_48_27 Train loss: 9.446715012018103e-06 at step: 37200 lr 0.0002
2024_08_10_09_49_09 Train loss: 2.3292257537832484e-05 at step: 37600 lr 0.0002
2024_08_10_09_49_50 Train loss: 0.00017954540089704096 at step: 38000 lr 0.0002
2024_08_10_09_50_32 Train loss: 5.809145932289539e-06 at step: 38400 lr 0.0002
2024_08_10_09_51_13 Train loss: 0.00012362029519863427 at step: 38800 lr 0.0002
2024_08_10_09_51_55 Train loss: 0.00022820031153969467 at step: 39200 lr 0.0002
2024_08_10_09_52_37 Train loss: 3.5141570151608903e-06 at step: 39600 lr 0.0002
2024_08_10_09_53_18 Train loss: 0.00033810798777267337 at step: 40000 lr 0.0002
2024_08_10_09_54_00 Train loss: 0.00018097581050824374 at step: 40400 lr 0.0002
(Val @ epoch 8) acc: 0.994875; ap: 0.9998012348676488
*************************
2024_08_10_09_54_37
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 91.7; ap: 98.2
(2 stylegan2 ) acc: 95.4; ap: 99.6
(3 biggan    ) acc: 81.4; ap: 87.4
(4 cyclegan  ) acc: 86.9; ap: 99.0
(5 stargan   ) acc: 99.9; ap: 100.0
(6 gaugan    ) acc: 71.6; ap: 75.8
(7 deepfake  ) acc: 66.8; ap: 69.1
(8 Mean      ) acc: 86.7; ap: 91.1
*************************
2024_08_10_09_58_41
2024_08_10_09_59_16 Train loss: 0.001500256359577179 at step: 40800 lr 0.0002
2024_08_10_09_59_57 Train loss: 3.2904816180234775e-05 at step: 41200 lr 0.0002
2024_08_10_10_00_39 Train loss: 0.0009038476855494082 at step: 41600 lr 0.0002
2024_08_10_10_01_21 Train loss: 0.0007876005256548524 at step: 42000 lr 0.0002
2024_08_10_10_02_03 Train loss: 0.00022512076247949153 at step: 42400 lr 0.0002
2024_08_10_10_02_44 Train loss: 0.0002021539257839322 at step: 42800 lr 0.0002
2024_08_10_10_03_26 Train loss: 5.58447209186852e-06 at step: 43200 lr 0.0002
2024_08_10_10_04_08 Train loss: 0.0003392853250261396 at step: 43600 lr 0.0002
2024_08_10_10_04_49 Train loss: 0.055989574640989304 at step: 44000 lr 0.0002
2024_08_10_10_05_31 Train loss: 0.0031073095742613077 at step: 44400 lr 0.0002
2024_08_10_10_06_12 Train loss: 1.4048693230961362e-07 at step: 44800 lr 0.0002
(Val @ epoch 9) acc: 0.992; ap: 0.9989241681515346
*************************
2024_08_10_10_07_00
(0 progan    ) acc: 99.4; ap: 100.0
(1 stylegan  ) acc: 92.2; ap: 98.9
(2 stylegan2 ) acc: 93.5; ap: 99.8
(3 biggan    ) acc: 76.2; ap: 86.6
(4 cyclegan  ) acc: 85.8; ap: 99.1
(5 stargan   ) acc: 99.6; ap: 100.0
(6 gaugan    ) acc: 62.3; ap: 70.4
(7 deepfake  ) acc: 79.8; ap: 87.4
(8 Mean      ) acc: 86.1; ap: 92.8
*************************
2024_08_10_10_11_06
2024_08_10_10_11_31 Train loss: 1.278452964470489e-05 at step: 45200 lr 0.0002
2024_08_10_10_12_13 Train loss: 5.480904292198829e-06 at step: 45600 lr 0.0002
2024_08_10_10_12_54 Train loss: 4.7467651711485814e-06 at step: 46000 lr 0.0002
2024_08_10_10_13_36 Train loss: 0.019468404352664948 at step: 46400 lr 0.0002
2024_08_10_10_14_18 Train loss: 1.2526069781415572e-07 at step: 46800 lr 0.0002
2024_08_10_10_14_59 Train loss: 2.5090144845307805e-05 at step: 47200 lr 0.0002
2024_08_10_10_15_41 Train loss: 4.1994149796664715e-05 at step: 47600 lr 0.0002
2024_08_10_10_16_23 Train loss: 2.2816095224698074e-05 at step: 48000 lr 0.0002
2024_08_10_10_17_04 Train loss: 1.8393771824776195e-05 at step: 48400 lr 0.0002
2024_08_10_10_17_46 Train loss: 0.0005083294818177819 at step: 48800 lr 0.0002
2024_08_10_10_18_28 Train loss: 0.00039350311271846294 at step: 49200 lr 0.0002
2024_08_10_10_19_01 changing lr at the end of epoch 10, iters 49511
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 10) acc: 0.99375; ap: 0.9998946154517643
*************************
2024_08_10_10_19_26
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 91.3; ap: 99.5
(2 stylegan2 ) acc: 93.9; ap: 99.8
(3 biggan    ) acc: 79.3; ap: 80.3
(4 cyclegan  ) acc: 92.5; ap: 99.0
(5 stargan   ) acc: 100.0; ap: 100.0
(6 gaugan    ) acc: 74.5; ap: 72.0
(7 deepfake  ) acc: 60.0; ap: 63.0
(8 Mean      ) acc: 86.4; ap: 89.2
*************************
2024_08_10_10_23_32
2024_08_10_10_23_47 Train loss: 0.015224481001496315 at step: 49600 lr 0.00018
2024_08_10_10_24_28 Train loss: 1.1987300467808382e-06 at step: 50000 lr 0.00018
2024_08_10_10_25_10 Train loss: 4.3842355808010325e-05 at step: 50400 lr 0.00018
2024_08_10_10_25_51 Train loss: 7.57036978029646e-05 at step: 50800 lr 0.00018
2024_08_10_10_26_33 Train loss: 9.173394209938124e-05 at step: 51200 lr 0.00018
2024_08_10_10_27_14 Train loss: 5.135173341841437e-05 at step: 51600 lr 0.00018
2024_08_10_10_27_56 Train loss: 0.0030895911622792482 at step: 52000 lr 0.00018
2024_08_10_10_28_38 Train loss: 6.1628170442418195e-06 at step: 52400 lr 0.00018
2024_08_10_10_29_19 Train loss: 1.0545139730311348e-06 at step: 52800 lr 0.00018
2024_08_10_10_30_01 Train loss: 7.15509543169901e-07 at step: 53200 lr 0.00018
2024_08_10_10_30_42 Train loss: 7.737385772088601e-07 at step: 53600 lr 0.00018
2024_08_10_10_31_25 Train loss: 1.307682123297127e-05 at step: 54000 lr 0.00018
(Val @ epoch 11) acc: 0.997875; ap: 0.9999570209428006
*************************
2024_08_10_10_31_52
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 89.3; ap: 96.7
(2 stylegan2 ) acc: 92.1; ap: 99.6
(3 biggan    ) acc: 79.5; ap: 86.8
(4 cyclegan  ) acc: 83.6; ap: 98.9
(5 stargan   ) acc: 83.8; ap: 100.0
(6 gaugan    ) acc: 72.9; ap: 74.6
(7 deepfake  ) acc: 61.4; ap: 76.6
(8 Mean      ) acc: 82.8; ap: 91.7
*************************
2024_08_10_10_35_58
2024_08_10_10_36_43 Train loss: 6.277523789322004e-05 at step: 54400 lr 0.00018
2024_08_10_10_37_25 Train loss: 2.2865389837534167e-05 at step: 54800 lr 0.00018
2024_08_10_10_38_07 Train loss: 0.00018017494585365057 at step: 55200 lr 0.00018
2024_08_10_10_38_48 Train loss: 5.249405512586236e-05 at step: 55600 lr 0.00018
2024_08_10_10_39_30 Train loss: 7.1096583269536495e-06 at step: 56000 lr 0.00018
2024_08_10_10_40_12 Train loss: 1.9634944692370482e-05 at step: 56400 lr 0.00018
2024_08_10_10_40_55 Train loss: 2.9942846595076844e-05 at step: 56800 lr 0.00018
2024_08_10_10_41_37 Train loss: 7.02541547070723e-06 at step: 57200 lr 0.00018
2024_08_10_10_42_18 Train loss: 3.440551381572732e-07 at step: 57600 lr 0.00018
2024_08_10_10_43_00 Train loss: 0.0002157880662707612 at step: 58000 lr 0.00018
2024_08_10_10_43_42 Train loss: 4.543039722193498e-06 at step: 58400 lr 0.00018
(Val @ epoch 12) acc: 0.995375; ap: 0.999817589175765
*************************
2024_08_10_10_44_19
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 90.3; ap: 99.0
(2 stylegan2 ) acc: 93.6; ap: 99.9
(3 biggan    ) acc: 82.6; ap: 90.3
(4 cyclegan  ) acc: 86.5; ap: 98.9
(5 stargan   ) acc: 99.6; ap: 100.0
(6 gaugan    ) acc: 75.0; ap: 78.9
(7 deepfake  ) acc: 58.4; ap: 65.7
(8 Mean      ) acc: 85.7; ap: 91.6
*************************
2024_08_10_10_48_20
2024_08_10_10_48_54 Train loss: 4.24616466432326e-08 at step: 58800 lr 0.00018
2024_08_10_10_49_36 Train loss: 4.285336035536602e-05 at step: 59200 lr 0.00018
2024_08_10_10_50_18 Train loss: 4.5213835164759075e-07 at step: 59600 lr 0.00018
2024_08_10_10_50_59 Train loss: 5.096464155940339e-05 at step: 60000 lr 0.00018
2024_08_10_10_51_41 Train loss: 0.0003195021126884967 at step: 60400 lr 0.00018
2024_08_10_10_52_23 Train loss: 8.30668523121858e-06 at step: 60800 lr 0.00018
2024_08_10_10_53_04 Train loss: 1.5318524674512446e-05 at step: 61200 lr 0.00018
2024_08_10_10_53_46 Train loss: 1.1574014024517965e-05 at step: 61600 lr 0.00018
2024_08_10_10_54_27 Train loss: 3.071087562034336e-08 at step: 62000 lr 0.00018
2024_08_10_10_55_09 Train loss: 5.619236844722764e-07 at step: 62400 lr 0.00018
2024_08_10_10_55_51 Train loss: 1.8685205986912479e-06 at step: 62800 lr 0.00018
(Val @ epoch 13) acc: 0.995625; ap: 0.9998631943490052
*************************
2024_08_10_10_56_38
(0 progan    ) acc: 99.6; ap: 100.0
(1 stylegan  ) acc: 90.9; ap: 98.8
(2 stylegan2 ) acc: 92.8; ap: 99.8
(3 biggan    ) acc: 84.5; ap: 91.9
(4 cyclegan  ) acc: 80.8; ap: 98.5
(5 stargan   ) acc: 99.4; ap: 100.0
(6 gaugan    ) acc: 79.0; ap: 85.7
(7 deepfake  ) acc: 67.0; ap: 69.4
(8 Mean      ) acc: 86.8; ap: 93.0
*************************
2024_08_10_11_00_45
2024_08_10_11_01_10 Train loss: 8.28518068374251e-07 at step: 63200 lr 0.00018
2024_08_10_11_01_51 Train loss: 0.001372167724184692 at step: 63600 lr 0.00018
2024_08_10_11_02_33 Train loss: 9.658467024564743e-05 at step: 64000 lr 0.00018
2024_08_10_11_03_15 Train loss: 7.450919656548649e-05 at step: 64400 lr 0.00018
2024_08_10_11_03_56 Train loss: 4.147080471739173e-05 at step: 64800 lr 0.00018
2024_08_10_11_04_38 Train loss: 0.0008541018469259143 at step: 65200 lr 0.00018
2024_08_10_11_05_20 Train loss: 7.17278726369841e-06 at step: 65600 lr 0.00018
2024_08_10_11_06_02 Train loss: 0.00017218833090737462 at step: 66000 lr 0.00018
2024_08_10_11_06_43 Train loss: 1.741319920256501e-06 at step: 66400 lr 0.00018
2024_08_10_11_07_25 Train loss: 0.00022815275588072836 at step: 66800 lr 0.00018
2024_08_10_11_08_07 Train loss: 2.333716793145868e-06 at step: 67200 lr 0.00018
(Val @ epoch 14) acc: 0.9965; ap: 0.9999424476872106
*************************
2024_08_10_11_09_06
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 89.9; ap: 97.4
(2 stylegan2 ) acc: 93.2; ap: 99.7
(3 biggan    ) acc: 80.7; ap: 86.5
(4 cyclegan  ) acc: 84.0; ap: 98.7
(5 stargan   ) acc: 97.7; ap: 100.0
(6 gaugan    ) acc: 77.0; ap: 74.7
(7 deepfake  ) acc: 70.3; ap: 73.0
(8 Mean      ) acc: 86.6; ap: 91.3
*************************
2024_08_10_11_13_12
2024_08_10_11_13_25 Train loss: 4.916346733807586e-05 at step: 67600 lr 0.00018
2024_08_10_11_14_07 Train loss: 0.0070608532987535 at step: 68000 lr 0.00018
2024_08_10_11_14_48 Train loss: 0.00017369557463098317 at step: 68400 lr 0.00018
2024_08_10_11_15_32 Train loss: 0.18857041001319885 at step: 68800 lr 0.00018
2024_08_10_11_16_13 Train loss: 8.185211481759325e-05 at step: 69200 lr 0.00018
2024_08_10_11_16_55 Train loss: 5.8842893224664294e-08 at step: 69600 lr 0.00018
2024_08_10_11_17_37 Train loss: 3.428127968163608e-07 at step: 70000 lr 0.00018
2024_08_10_11_18_18 Train loss: 0.002784162759780884 at step: 70400 lr 0.00018
2024_08_10_11_19_00 Train loss: 1.3428357306111138e-06 at step: 70800 lr 0.00018
2024_08_10_11_19_41 Train loss: 1.4305443301054765e-06 at step: 71200 lr 0.00018
2024_08_10_11_20_23 Train loss: 1.936548876813049e-08 at step: 71600 lr 0.00018
2024_08_10_11_21_05 Train loss: 3.967809618643514e-07 at step: 72000 lr 0.00018
(Val @ epoch 15) acc: 0.9975; ap: 0.9999528868002798
*************************
2024_08_10_11_21_34
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.8; ap: 99.6
(2 stylegan2 ) acc: 96.0; ap: 99.9
(3 biggan    ) acc: 83.8; ap: 89.3
(4 cyclegan  ) acc: 85.1; ap: 98.7
(5 stargan   ) acc: 99.8; ap: 100.0
(6 gaugan    ) acc: 78.2; ap: 74.8
(7 deepfake  ) acc: 54.7; ap: 64.3
(8 Mean      ) acc: 86.1; ap: 90.8
*************************
2024_08_10_11_25_42
2024_08_10_11_26_27 Train loss: 1.940953097800957e-06 at step: 72400 lr 0.00018
2024_08_10_11_27_09 Train loss: 3.88116541216732e-06 at step: 72800 lr 0.00018
2024_08_10_11_27_50 Train loss: 0.0004178985545877367 at step: 73200 lr 0.00018
2024_08_10_11_28_32 Train loss: 0.0005272087873890996 at step: 73600 lr 0.00018
2024_08_10_11_29_14 Train loss: 4.007095776614733e-05 at step: 74000 lr 0.00018
2024_08_10_11_29_56 Train loss: 1.1070571304117038e-07 at step: 74400 lr 0.00018
2024_08_10_11_30_37 Train loss: 6.801821291446686e-05 at step: 74800 lr 0.00018
2024_08_10_11_31_19 Train loss: 3.54027264393153e-07 at step: 75200 lr 0.00018
2024_08_10_11_32_00 Train loss: 1.5790917184954623e-10 at step: 75600 lr 0.00018
2024_08_10_11_32_43 Train loss: 1.5601508494000882e-05 at step: 76000 lr 0.00018
2024_08_10_11_33_24 Train loss: 1.1027046866729506e-06 at step: 76400 lr 0.00018
(Val @ epoch 16) acc: 0.997; ap: 0.9999463289025993
*************************
2024_08_10_11_34_02
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.2; ap: 98.5
(2 stylegan2 ) acc: 95.7; ap: 99.8
(3 biggan    ) acc: 78.5; ap: 90.9
(4 cyclegan  ) acc: 77.5; ap: 98.7
(5 stargan   ) acc: 99.3; ap: 100.0
(6 gaugan    ) acc: 69.0; ap: 84.6
(7 deepfake  ) acc: 69.5; ap: 74.3
(8 Mean      ) acc: 85.2; ap: 93.3
*************************
2024_08_10_11_38_08
2024_08_10_11_38_42 Train loss: 0.0005628397921100259 at step: 76800 lr 0.00018
2024_08_10_11_39_24 Train loss: 1.4642039332102286e-06 at step: 77200 lr 0.00018
2024_08_10_11_40_05 Train loss: 1.5199188965198118e-06 at step: 77600 lr 0.00018
2024_08_10_11_40_47 Train loss: 1.3427368550367191e-08 at step: 78000 lr 0.00018
2024_08_10_11_41_29 Train loss: 3.75072423206696e-14 at step: 78400 lr 0.00018
2024_08_10_11_42_10 Train loss: 2.3181666620075703e-05 at step: 78800 lr 0.00018
2024_08_10_11_42_52 Train loss: 0.001333254505880177 at step: 79200 lr 0.00018
2024_08_10_11_43_34 Train loss: 7.16937574907206e-05 at step: 79600 lr 0.00018
2024_08_10_11_44_16 Train loss: 3.871838271152228e-05 at step: 80000 lr 0.00018
2024_08_10_11_44_57 Train loss: 0.022382985800504684 at step: 80400 lr 0.00018
2024_08_10_11_45_39 Train loss: 5.189937724026095e-07 at step: 80800 lr 0.00018
(Val @ epoch 17) acc: 0.993375; ap: 0.9996168889164022
*************************
2024_08_10_11_46_29
(0 progan    ) acc: 99.5; ap: 100.0
(1 stylegan  ) acc: 91.4; ap: 98.9
(2 stylegan2 ) acc: 93.8; ap: 99.9
(3 biggan    ) acc: 80.0; ap: 90.9
(4 cyclegan  ) acc: 79.0; ap: 98.9
(5 stargan   ) acc: 98.3; ap: 100.0
(6 gaugan    ) acc: 68.7; ap: 81.3
(7 deepfake  ) acc: 64.8; ap: 63.0
(8 Mean      ) acc: 84.4; ap: 91.6
*************************
2024_08_10_11_50_32
2024_08_10_11_50_56 Train loss: 3.609606937970966e-05 at step: 81200 lr 0.00018
2024_08_10_11_51_37 Train loss: 1.4913632639945718e-07 at step: 81600 lr 0.00018
2024_08_10_11_52_19 Train loss: 1.5497212189075071e-06 at step: 82000 lr 0.00018
2024_08_10_11_53_01 Train loss: 1.9518066096679831e-07 at step: 82400 lr 0.00018
2024_08_10_11_53_42 Train loss: 3.259544973843731e-05 at step: 82800 lr 0.00018
2024_08_10_11_54_24 Train loss: 8.578314736951143e-05 at step: 83200 lr 0.00018
2024_08_10_11_55_05 Train loss: 3.128321986878291e-05 at step: 83600 lr 0.00018
2024_08_10_11_55_47 Train loss: 2.743171080510365e-06 at step: 84000 lr 0.00018
2024_08_10_11_56_29 Train loss: 3.554627255653031e-06 at step: 84400 lr 0.00018
2024_08_10_11_57_10 Train loss: 3.5435787140158936e-06 at step: 84800 lr 0.00018
2024_08_10_11_57_52 Train loss: 9.69159955275245e-05 at step: 85200 lr 0.00018
(Val @ epoch 18) acc: 0.99725; ap: 0.9999276261727905
*************************
2024_08_10_11_58_51
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.8; ap: 99.5
(2 stylegan2 ) acc: 97.1; ap: 100.0
(3 biggan    ) acc: 84.5; ap: 90.3
(4 cyclegan  ) acc: 84.6; ap: 98.9
(5 stargan   ) acc: 98.6; ap: 100.0
(6 gaugan    ) acc: 81.2; ap: 78.4
(7 deepfake  ) acc: 65.7; ap: 76.7
(8 Mean      ) acc: 88.0; ap: 93.0
*************************
2024_08_10_12_02_55
2024_08_10_12_03_08 Train loss: 2.4408129320363514e-05 at step: 85600 lr 0.00018
2024_08_10_12_03_50 Train loss: 1.2295967053432832e-06 at step: 86000 lr 0.00018
2024_08_10_12_04_32 Train loss: 3.880748772644438e-05 at step: 86400 lr 0.00018
2024_08_10_12_05_14 Train loss: 2.239480409116368e-06 at step: 86800 lr 0.00018
2024_08_10_12_05_56 Train loss: 2.606299460694572e-07 at step: 87200 lr 0.00018
2024_08_10_12_06_37 Train loss: 1.5215453004202573e-06 at step: 87600 lr 0.00018
2024_08_10_12_07_19 Train loss: 3.678643452076358e-07 at step: 88000 lr 0.00018
2024_08_10_12_08_01 Train loss: 2.079474228366962e-07 at step: 88400 lr 0.00018
2024_08_10_12_08_42 Train loss: 1.5033776890049921e-07 at step: 88800 lr 0.00018
2024_08_10_12_09_24 Train loss: 1.1843372504927174e-07 at step: 89200 lr 0.00018
2024_08_10_12_10_06 Train loss: 0.0012847622856497765 at step: 89600 lr 0.00018
2024_08_10_12_10_47 Train loss: 0.00012987425725441426 at step: 90000 lr 0.00018
(Val @ epoch 19) acc: 0.993375; ap: 0.999644283700486
*************************
2024_08_10_12_11_16
(0 progan    ) acc: 99.5; ap: 100.0
(1 stylegan  ) acc: 92.9; ap: 99.3
(2 stylegan2 ) acc: 94.2; ap: 99.8
(3 biggan    ) acc: 86.0; ap: 93.1
(4 cyclegan  ) acc: 89.5; ap: 98.9
(5 stargan   ) acc: 99.7; ap: 100.0
(6 gaugan    ) acc: 83.8; ap: 88.1
(7 deepfake  ) acc: 69.7; ap: 73.9
(8 Mean      ) acc: 89.4; ap: 94.1
*************************
2024_08_10_12_15_18
2024_08_10_12_16_02 Train loss: 2.7530066404324316e-07 at step: 90400 lr 0.00018
2024_08_10_12_16_43 Train loss: 1.7151852205188334e-07 at step: 90800 lr 0.00018
2024_08_10_12_17_25 Train loss: 9.839837389336026e-08 at step: 91200 lr 0.00018
2024_08_10_12_18_07 Train loss: 7.100126042658417e-10 at step: 91600 lr 0.00018
2024_08_10_12_18_48 Train loss: 6.209742423379794e-06 at step: 92000 lr 0.00018
2024_08_10_12_19_30 Train loss: 1.0781420911598616e-07 at step: 92400 lr 0.00018
2024_08_10_12_20_12 Train loss: 0.23901605606079102 at step: 92800 lr 0.00018
2024_08_10_12_20_54 Train loss: 3.874316462315619e-06 at step: 93200 lr 0.00018
2024_08_10_12_21_35 Train loss: 5.855603831150802e-06 at step: 93600 lr 0.00018
2024_08_10_12_22_17 Train loss: 4.947216893924633e-06 at step: 94000 lr 0.00018
2024_08_10_12_22_58 Train loss: 1.15028608860257e-09 at step: 94400 lr 0.00018
saving the model at the end of epoch 20, iters 94521
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_latest.pth
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_20.pth
2024_08_10_12_23_12 changing lr at the end of epoch 20, iters 94521
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 20) acc: 0.996; ap: 0.9999351108764324
*************************
2024_08_10_12_23_37
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 90.3; ap: 97.2
(2 stylegan2 ) acc: 94.3; ap: 99.8
(3 biggan    ) acc: 68.2; ap: 86.9
(4 cyclegan  ) acc: 66.5; ap: 99.1
(5 stargan   ) acc: 77.1; ap: 99.9
(6 gaugan    ) acc: 56.5; ap: 78.4
(7 deepfake  ) acc: 63.8; ap: 68.9
(8 Mean      ) acc: 77.0; ap: 91.3
*************************
2024_08_10_12_27_47
2024_08_10_12_28_20 Train loss: 1.7447653590352274e-05 at step: 94800 lr 0.000162
2024_08_10_12_29_02 Train loss: 1.4632946658821311e-05 at step: 95200 lr 0.000162
2024_08_10_12_29_44 Train loss: 5.181646338314749e-05 at step: 95600 lr 0.000162
2024_08_10_12_30_26 Train loss: 8.045102731557563e-06 at step: 96000 lr 0.000162
2024_08_10_12_31_08 Train loss: 8.605883863310737e-07 at step: 96400 lr 0.000162
2024_08_10_12_31_50 Train loss: 3.6412361623661127e-06 at step: 96800 lr 0.000162
2024_08_10_12_32_31 Train loss: 8.946458507352872e-08 at step: 97200 lr 0.000162
2024_08_10_12_33_13 Train loss: 3.3460531567719443e-10 at step: 97600 lr 0.000162
2024_08_10_12_33_55 Train loss: 1.0791950444399845e-06 at step: 98000 lr 0.000162
2024_08_10_12_34_36 Train loss: 8.942211593421234e-08 at step: 98400 lr 0.000162
2024_08_10_12_35_18 Train loss: 3.1189252069907525e-08 at step: 98800 lr 0.000162
(Val @ epoch 21) acc: 0.99575; ap: 0.9999670485102516
*************************
2024_08_10_12_36_07
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 88.8; ap: 98.3
(2 stylegan2 ) acc: 93.4; ap: 99.9
(3 biggan    ) acc: 68.8; ap: 88.4
(4 cyclegan  ) acc: 55.6; ap: 98.1
(5 stargan   ) acc: 66.9; ap: 99.9
(6 gaugan    ) acc: 59.1; ap: 83.3
(7 deepfake  ) acc: 60.9; ap: 64.5
(8 Mean      ) acc: 74.2; ap: 91.6
*************************
2024_08_10_12_40_08
2024_08_10_12_40_31 Train loss: 2.3231258694522694e-08 at step: 99200 lr 0.000162
2024_08_10_12_41_13 Train loss: 2.0396701074787416e-06 at step: 99600 lr 0.000162
2024_08_10_12_41_55 Train loss: 0.00012588713434524834 at step: 100000 lr 0.000162
2024_08_10_12_42_36 Train loss: 1.8823702703230083e-06 at step: 100400 lr 0.000162
2024_08_10_12_43_18 Train loss: 2.771615982055664e-06 at step: 100800 lr 0.000162
2024_08_10_12_44_00 Train loss: 3.1107811082620174e-05 at step: 101200 lr 0.000162
2024_08_10_12_44_41 Train loss: 0.00018557981820777059 at step: 101600 lr 0.000162
2024_08_10_12_45_23 Train loss: 1.1460988957878726e-07 at step: 102000 lr 0.000162
2024_08_10_12_46_05 Train loss: 2.0271704670449253e-06 at step: 102400 lr 0.000162
2024_08_10_12_46_48 Train loss: 1.3106650840200018e-06 at step: 102800 lr 0.000162
2024_08_10_12_47_30 Train loss: 3.0108775916914965e-08 at step: 103200 lr 0.000162
(Val @ epoch 22) acc: 0.997; ap: 0.9999588718102767
*************************
2024_08_10_12_48_30
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.7; ap: 98.1
(2 stylegan2 ) acc: 96.5; ap: 99.9
(3 biggan    ) acc: 75.8; ap: 88.2
(4 cyclegan  ) acc: 68.0; ap: 98.1
(5 stargan   ) acc: 87.8; ap: 100.0
(6 gaugan    ) acc: 69.1; ap: 81.6
(7 deepfake  ) acc: 68.4; ap: 75.3
(8 Mean      ) acc: 82.1; ap: 92.7
*************************
2024_08_10_12_52_34
2024_08_10_12_52_48 Train loss: 3.3837595765362494e-06 at step: 103600 lr 0.000162
2024_08_10_12_53_30 Train loss: 0.0712367445230484 at step: 104000 lr 0.000162
2024_08_10_12_54_12 Train loss: 6.013453003106406e-06 at step: 104400 lr 0.000162
2024_08_10_12_54_53 Train loss: 2.98025327083451e-07 at step: 104800 lr 0.000162
2024_08_10_12_55_35 Train loss: 5.468852037893157e-08 at step: 105200 lr 0.000162
2024_08_10_12_56_17 Train loss: 3.2533607736695558e-06 at step: 105600 lr 0.000162
2024_08_10_12_56_58 Train loss: 3.974096216552425e-06 at step: 106000 lr 0.000162
2024_08_10_12_57_40 Train loss: 2.198645915996167e-06 at step: 106400 lr 0.000162
2024_08_10_12_58_22 Train loss: 1.1309743968013208e-05 at step: 106800 lr 0.000162
2024_08_10_12_59_03 Train loss: 5.130486533744261e-05 at step: 107200 lr 0.000162
2024_08_10_12_59_45 Train loss: 4.5258311729412526e-05 at step: 107600 lr 0.000162
2024_08_10_13_00_27 Train loss: 3.1673639568907674e-06 at step: 108000 lr 0.000162
(Val @ epoch 23) acc: 0.998375; ap: 0.9999872941752798
*************************
2024_08_10_13_00_56
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 93.3; ap: 99.6
(2 stylegan2 ) acc: 95.8; ap: 99.9
(3 biggan    ) acc: 77.9; ap: 91.2
(4 cyclegan  ) acc: 68.8; ap: 98.4
(5 stargan   ) acc: 94.4; ap: 100.0
(6 gaugan    ) acc: 70.4; ap: 84.9
(7 deepfake  ) acc: 69.3; ap: 78.4
(8 Mean      ) acc: 83.7; ap: 94.0
*************************
2024_08_10_13_05_02
2024_08_10_13_05_46 Train loss: 5.079572929389542e-06 at step: 108400 lr 0.000162
2024_08_10_13_06_27 Train loss: 6.97815339663066e-05 at step: 108800 lr 0.000162
2024_08_10_13_07_09 Train loss: 5.507437208507326e-07 at step: 109200 lr 0.000162
2024_08_10_13_07_51 Train loss: 4.738571078632958e-06 at step: 109600 lr 0.000162
2024_08_10_13_08_32 Train loss: 2.3832940314605366e-06 at step: 110000 lr 0.000162
2024_08_10_13_09_14 Train loss: 1.7633736206335016e-05 at step: 110400 lr 0.000162
2024_08_10_13_09_56 Train loss: 5.865927960257977e-05 at step: 110800 lr 0.000162
2024_08_10_13_10_37 Train loss: 1.5537226545347949e-06 at step: 111200 lr 0.000162
2024_08_10_13_11_19 Train loss: 2.9014350921841014e-09 at step: 111600 lr 0.000162
2024_08_10_13_12_01 Train loss: 2.425187085464131e-05 at step: 112000 lr 0.000162
2024_08_10_13_12_43 Train loss: 8.082071190074203e-07 at step: 112400 lr 0.000162
(Val @ epoch 24) acc: 0.990875; ap: 0.9999776508094397
*************************
2024_08_10_13_13_25
(0 progan    ) acc: 99.3; ap: 100.0
(1 stylegan  ) acc: 92.1; ap: 99.8
(2 stylegan2 ) acc: 90.7; ap: 99.6
(3 biggan    ) acc: 80.0; ap: 92.8
(4 cyclegan  ) acc: 75.9; ap: 99.0
(5 stargan   ) acc: 99.7; ap: 100.0
(6 gaugan    ) acc: 84.0; ap: 91.3
(7 deepfake  ) acc: 66.8; ap: 73.5
(8 Mean      ) acc: 86.1; ap: 94.5
*************************
2024_08_10_13_17_31
2024_08_10_13_18_04 Train loss: 4.513459543886711e-07 at step: 112800 lr 0.000162
2024_08_10_13_18_46 Train loss: 3.172268492335206e-08 at step: 113200 lr 0.000162
2024_08_10_13_19_27 Train loss: 9.468780604038329e-07 at step: 113600 lr 0.000162
2024_08_10_13_20_09 Train loss: 6.64101840186504e-09 at step: 114000 lr 0.000162
2024_08_10_13_20_51 Train loss: 1.6115250218717847e-06 at step: 114400 lr 0.000162
2024_08_10_13_21_32 Train loss: 7.476913185811007e-12 at step: 114800 lr 0.000162
2024_08_10_13_22_14 Train loss: 1.0405543093838787e-08 at step: 115200 lr 0.000162
2024_08_10_13_22_56 Train loss: 0.0002780236827675253 at step: 115600 lr 0.000162
2024_08_10_13_23_41 Train loss: 0.007585898507386446 at step: 116000 lr 0.000162
2024_08_10_13_24_26 Train loss: 2.1424235718825457e-09 at step: 116400 lr 0.000162
2024_08_10_13_25_11 Train loss: 1.9380904632271267e-06 at step: 116800 lr 0.000162
(Val @ epoch 25) acc: 0.9965; ap: 0.9998476240104587
*************************
2024_08_10_13_26_01
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.9; ap: 99.4
(2 stylegan2 ) acc: 98.1; ap: 100.0
(3 biggan    ) acc: 81.2; ap: 87.4
(4 cyclegan  ) acc: 84.3; ap: 98.6
(5 stargan   ) acc: 98.4; ap: 100.0
(6 gaugan    ) acc: 83.4; ap: 82.5
(7 deepfake  ) acc: 60.9; ap: 63.3
(8 Mean      ) acc: 87.4; ap: 91.4
*************************
2024_08_10_13_30_12
2024_08_10_13_30_35 Train loss: 1.2527805210993392e-06 at step: 117200 lr 0.000162
2024_08_10_13_31_17 Train loss: 0.004797191824764013 at step: 117600 lr 0.000162
2024_08_10_13_31_59 Train loss: 1.1614366712819901e-07 at step: 118000 lr 0.000162
2024_08_10_13_32_40 Train loss: 6.86519729953261e-11 at step: 118400 lr 0.000162
2024_08_10_13_33_22 Train loss: 0.002388991415500641 at step: 118800 lr 0.000162
2024_08_10_13_34_04 Train loss: 2.026577249125694e-06 at step: 119200 lr 0.000162
2024_08_10_13_34_45 Train loss: 0.00011860607628477737 at step: 119600 lr 0.000162
2024_08_10_13_35_27 Train loss: 4.8799115789921466e-12 at step: 120000 lr 0.000162
2024_08_10_13_36_10 Train loss: 6.308221145445714e-08 at step: 120400 lr 0.000162
2024_08_10_13_36_52 Train loss: 2.0640934963012114e-05 at step: 120800 lr 0.000162
2024_08_10_13_37_33 Train loss: 2.1614857459439918e-08 at step: 121200 lr 0.000162
(Val @ epoch 26) acc: 0.99725; ap: 0.9999844820383109
*************************
2024_08_10_13_38_38
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.7; ap: 99.6
(2 stylegan2 ) acc: 94.2; ap: 99.9
(3 biggan    ) acc: 81.6; ap: 90.2
(4 cyclegan  ) acc: 77.5; ap: 98.8
(5 stargan   ) acc: 98.6; ap: 100.0
(6 gaugan    ) acc: 79.0; ap: 78.9
(7 deepfake  ) acc: 64.3; ap: 66.3
(8 Mean      ) acc: 85.8; ap: 91.7
*************************
2024_08_10_13_42_49
2024_08_10_13_43_01 Train loss: 1.0738375522123533e-06 at step: 121600 lr 0.000162
2024_08_10_13_43_45 Train loss: 0.0011075208894908428 at step: 122000 lr 0.000162
2024_08_10_13_44_27 Train loss: 0.00020856509217992425 at step: 122400 lr 0.000162
2024_08_10_13_45_09 Train loss: 3.49177128100564e-07 at step: 122800 lr 0.000162
2024_08_10_13_45_50 Train loss: 0.0001451787684345618 at step: 123200 lr 0.000162
2024_08_10_13_46_32 Train loss: 1.7008343888846866e-07 at step: 123600 lr 0.000162
2024_08_10_13_47_14 Train loss: 1.4958015981392236e-07 at step: 124000 lr 0.000162
2024_08_10_13_47_55 Train loss: 1.4313998235593317e-06 at step: 124400 lr 0.000162
2024_08_10_13_48_37 Train loss: 3.2782554626464844e-07 at step: 124800 lr 0.000162
2024_08_10_13_49_19 Train loss: 3.712592160809436e-06 at step: 125200 lr 0.000162
2024_08_10_13_50_00 Train loss: 1.4208453080755135e-07 at step: 125600 lr 0.000162
2024_08_10_13_50_42 Train loss: 4.232912909074571e-10 at step: 126000 lr 0.000162
(Val @ epoch 27) acc: 0.997; ap: 0.9999379435309271
*************************
2024_08_10_13_51_12
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 99.4
(2 stylegan2 ) acc: 96.2; ap: 100.0
(3 biggan    ) acc: 81.4; ap: 90.5
(4 cyclegan  ) acc: 74.0; ap: 98.7
(5 stargan   ) acc: 98.9; ap: 100.0
(6 gaugan    ) acc: 81.5; ap: 85.6
(7 deepfake  ) acc: 68.5; ap: 74.2
(8 Mean      ) acc: 86.6; ap: 93.6
*************************
2024_08_10_13_55_17
2024_08_10_13_56_00 Train loss: 2.89231538772583e-05 at step: 126400 lr 0.000162
2024_08_10_13_56_42 Train loss: 1.039129955415774e-07 at step: 126800 lr 0.000162
2024_08_10_13_57_23 Train loss: 0.005173237528651953 at step: 127200 lr 0.000162
2024_08_10_13_58_05 Train loss: 2.1179650389058224e-07 at step: 127600 lr 0.000162
2024_08_10_13_58_46 Train loss: 3.22262039276211e-08 at step: 128000 lr 0.000162
2024_08_10_13_59_28 Train loss: 1.6684458259419443e-09 at step: 128400 lr 0.000162
2024_08_10_14_00_10 Train loss: 5.334623892849777e-06 at step: 128800 lr 0.000162
2024_08_10_14_00_52 Train loss: 8.941007081375574e-07 at step: 129200 lr 0.000162
2024_08_10_14_01_33 Train loss: 3.9608294173376635e-06 at step: 129600 lr 0.000162
2024_08_10_14_02_15 Train loss: 4.2021338231279515e-06 at step: 130000 lr 0.000162
2024_08_10_14_02_56 Train loss: 4.06293416688186e-08 at step: 130400 lr 0.000162
(Val @ epoch 28) acc: 0.997125; ap: 0.9999211262050426
*************************
2024_08_10_14_03_40
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.2; ap: 99.3
(2 stylegan2 ) acc: 92.8; ap: 99.6
(3 biggan    ) acc: 81.9; ap: 88.2
(4 cyclegan  ) acc: 80.4; ap: 98.2
(5 stargan   ) acc: 97.2; ap: 99.9
(6 gaugan    ) acc: 83.8; ap: 80.7
(7 deepfake  ) acc: 62.8; ap: 63.4
(8 Mean      ) acc: 86.6; ap: 91.2
*************************
2024_08_10_14_07_50
2024_08_10_14_08_22 Train loss: 0.00038649383350275457 at step: 130800 lr 0.000162
2024_08_10_14_09_04 Train loss: 9.253288908439572e-07 at step: 131200 lr 0.000162
2024_08_10_14_09_46 Train loss: 4.172366516286274e-06 at step: 131600 lr 0.000162
2024_08_10_14_10_29 Train loss: 4.82376435684273e-06 at step: 132000 lr 0.000162
2024_08_10_14_11_15 Train loss: 3.393071665414027e-06 at step: 132400 lr 0.000162
2024_08_10_14_12_02 Train loss: 2.0772218704223633e-05 at step: 132800 lr 0.000162
2024_08_10_14_12_48 Train loss: 5.407953267422272e-06 at step: 133200 lr 0.000162
2024_08_10_14_13_32 Train loss: 1.1182849448232446e-05 at step: 133600 lr 0.000162
2024_08_10_14_14_14 Train loss: 6.664246825494047e-07 at step: 134000 lr 0.000162
2024_08_10_14_14_55 Train loss: 7.032876601442695e-05 at step: 134400 lr 0.000162
2024_08_10_14_15_37 Train loss: 0.0002212542312918231 at step: 134800 lr 0.000162
(Val @ epoch 29) acc: 0.996875; ap: 0.9999797228564713
*************************
2024_08_10_14_16_28
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.0; ap: 99.4
(2 stylegan2 ) acc: 92.1; ap: 99.9
(3 biggan    ) acc: 75.9; ap: 92.6
(4 cyclegan  ) acc: 64.1; ap: 98.9
(5 stargan   ) acc: 82.9; ap: 100.0
(6 gaugan    ) acc: 64.2; ap: 87.0
(7 deepfake  ) acc: 64.0; ap: 66.7
(8 Mean      ) acc: 79.3; ap: 93.0
*************************
2024_08_10_14_20_36
2024_08_10_14_20_59 Train loss: 3.465189820417436e-06 at step: 135200 lr 0.000162
2024_08_10_14_21_41 Train loss: 2.98027771350462e-08 at step: 135600 lr 0.000162
2024_08_10_14_22_22 Train loss: 5.800203553008032e-07 at step: 136000 lr 0.000162
2024_08_10_14_23_04 Train loss: 1.1225124996183666e-10 at step: 136400 lr 0.000162
2024_08_10_14_23_46 Train loss: 1.1175144209119026e-05 at step: 136800 lr 0.000162
2024_08_10_14_24_27 Train loss: 2.4866966441550176e-07 at step: 137200 lr 0.000162
2024_08_10_14_25_09 Train loss: 2.345442771911621e-05 at step: 137600 lr 0.000162
2024_08_10_14_25_51 Train loss: 0.00024232234864030033 at step: 138000 lr 0.000162
2024_08_10_14_26_33 Train loss: 0.00013896863674744964 at step: 138400 lr 0.000162
2024_08_10_14_27_14 Train loss: 1.825392246246338e-05 at step: 138800 lr 0.000162
2024_08_10_14_27_56 Train loss: 1.811667743822909e-06 at step: 139200 lr 0.000162
2024_08_10_14_28_31 changing lr at the end of epoch 30, iters 139531
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 30) acc: 0.9965; ap: 0.9999747908295891
*************************
2024_08_10_14_28_59
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 89.9; ap: 98.8
(2 stylegan2 ) acc: 93.3; ap: 100.0
(3 biggan    ) acc: 73.5; ap: 88.6
(4 cyclegan  ) acc: 67.8; ap: 98.3
(5 stargan   ) acc: 86.7; ap: 100.0
(6 gaugan    ) acc: 66.4; ap: 80.7
(7 deepfake  ) acc: 65.1; ap: 70.2
(8 Mean      ) acc: 80.3; ap: 92.1
*************************
2024_08_10_14_33_03
2024_08_10_14_33_16 Train loss: 9.344478257844457e-08 at step: 139600 lr 0.00014580000000000002
2024_08_10_14_33_57 Train loss: 0.0010172873735427856 at step: 140000 lr 0.00014580000000000002
2024_08_10_14_34_41 Train loss: 4.2408112221892225e-07 at step: 140400 lr 0.00014580000000000002
2024_08_10_14_35_23 Train loss: 3.3234253127290003e-09 at step: 140800 lr 0.00014580000000000002
2024_08_10_14_36_05 Train loss: 6.261212206482014e-07 at step: 141200 lr 0.00014580000000000002
2024_08_10_14_36_50 Train loss: 4.0347316598854377e-07 at step: 141600 lr 0.00014580000000000002
2024_08_10_14_37_32 Train loss: 6.53301839292908e-08 at step: 142000 lr 0.00014580000000000002
2024_08_10_14_38_14 Train loss: 4.958214958605822e-06 at step: 142400 lr 0.00014580000000000002
2024_08_10_14_38_55 Train loss: 1.8092723621521145e-05 at step: 142800 lr 0.00014580000000000002
2024_08_10_14_39_37 Train loss: 3.994248345406959e-07 at step: 143200 lr 0.00014580000000000002
2024_08_10_14_40_19 Train loss: 0.00017016033234540373 at step: 143600 lr 0.00014580000000000002
2024_08_10_14_41_00 Train loss: 3.5813660360872746e-05 at step: 144000 lr 0.00014580000000000002
(Val @ epoch 31) acc: 0.995375; ap: 0.9997430280243703
*************************
2024_08_10_14_41_30
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.2; ap: 99.8
(2 stylegan2 ) acc: 94.3; ap: 99.9
(3 biggan    ) acc: 85.3; ap: 90.9
(4 cyclegan  ) acc: 81.9; ap: 97.9
(5 stargan   ) acc: 99.6; ap: 100.0
(6 gaugan    ) acc: 84.7; ap: 88.2
(7 deepfake  ) acc: 66.8; ap: 72.9
(8 Mean      ) acc: 88.3; ap: 93.7
*************************
2024_08_10_14_45_33
2024_08_10_14_46_16 Train loss: 4.346054183201886e-09 at step: 144400 lr 0.00014580000000000002
2024_08_10_14_46_58 Train loss: 4.1760014823921665e-07 at step: 144800 lr 0.00014580000000000002
2024_08_10_14_47_40 Train loss: 6.357338861562312e-06 at step: 145200 lr 0.00014580000000000002
2024_08_10_14_48_21 Train loss: 0.0012802481651306152 at step: 145600 lr 0.00014580000000000002
2024_08_10_14_49_03 Train loss: 1.530667213955894e-05 at step: 146000 lr 0.00014580000000000002
2024_08_10_14_49_45 Train loss: 2.9802322387695312e-06 at step: 146400 lr 0.00014580000000000002
2024_08_10_14_50_29 Train loss: 4.337832706369227e-06 at step: 146800 lr 0.00014580000000000002
2024_08_10_14_51_15 Train loss: 5.364418029785156e-06 at step: 147200 lr 0.00014580000000000002
2024_08_10_14_52_01 Train loss: 0.0004987326683476567 at step: 147600 lr 0.00014580000000000002
2024_08_10_14_52_47 Train loss: 5.237213372311089e-06 at step: 148000 lr 0.00014580000000000002
2024_08_10_14_53_29 Train loss: 1.8550646245785174e-06 at step: 148400 lr 0.00014580000000000002
(Val @ epoch 32) acc: 0.9975; ap: 0.9999306599254804
*************************
2024_08_10_14_54_10
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 91.3; ap: 98.8
(2 stylegan2 ) acc: 95.0; ap: 99.9
(3 biggan    ) acc: 78.0; ap: 88.3
(4 cyclegan  ) acc: 73.4; ap: 98.7
(5 stargan   ) acc: 84.6; ap: 100.0
(6 gaugan    ) acc: 73.6; ap: 80.2
(7 deepfake  ) acc: 65.6; ap: 72.9
(8 Mean      ) acc: 82.7; ap: 92.3
*************************
2024_08_10_14_58_24
2024_08_10_14_58_58 Train loss: 6.058363055672089e-08 at step: 148800 lr 0.00014580000000000002
2024_08_10_14_59_39 Train loss: 7.540131718997145e-06 at step: 149200 lr 0.00014580000000000002
2024_08_10_15_00_21 Train loss: 1.505339582763554e-06 at step: 149600 lr 0.00014580000000000002
2024_08_10_15_01_02 Train loss: 3.9676911001151893e-07 at step: 150000 lr 0.00014580000000000002
2024_08_10_15_01_44 Train loss: 1.3306834262039047e-05 at step: 150400 lr 0.00014580000000000002
2024_08_10_15_02_26 Train loss: 8.510755833412986e-08 at step: 150800 lr 0.00014580000000000002
2024_08_10_15_03_07 Train loss: 0.0010006725788116455 at step: 151200 lr 0.00014580000000000002
2024_08_10_15_03_49 Train loss: 6.490029846872858e-08 at step: 151600 lr 0.00014580000000000002
2024_08_10_15_04_31 Train loss: 5.988196107864496e-07 at step: 152000 lr 0.00014580000000000002
2024_08_10_15_05_12 Train loss: 0.0005355514585971832 at step: 152400 lr 0.00014580000000000002
2024_08_10_15_05_54 Train loss: 1.5079742752277525e-06 at step: 152800 lr 0.00014580000000000002
(Val @ epoch 33) acc: 0.996625; ap: 0.9998628454543568
*************************
2024_08_10_15_06_46
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.0; ap: 99.8
(2 stylegan2 ) acc: 96.3; ap: 99.9
(3 biggan    ) acc: 77.8; ap: 91.2
(4 cyclegan  ) acc: 74.2; ap: 98.8
(5 stargan   ) acc: 95.4; ap: 100.0
(6 gaugan    ) acc: 72.0; ap: 87.2
(7 deepfake  ) acc: 76.4; ap: 86.9
(8 Mean      ) acc: 85.8; ap: 95.5
*************************
2024_08_10_15_10_53
2024_08_10_15_11_18 Train loss: 2.9840755644272576e-08 at step: 153200 lr 0.00014580000000000002
2024_08_10_15_12_00 Train loss: 3.7672052144444024e-07 at step: 153600 lr 0.00014580000000000002
2024_08_10_15_12_42 Train loss: 1.2243338787243374e-08 at step: 154000 lr 0.00014580000000000002
2024_08_10_15_13_25 Train loss: 2.196385224806363e-07 at step: 154400 lr 0.00014580000000000002
2024_08_10_15_14_08 Train loss: 8.102376369834019e-08 at step: 154800 lr 0.00014580000000000002
2024_08_10_15_14_51 Train loss: 5.5159769544843584e-05 at step: 155200 lr 0.00014580000000000002
2024_08_10_15_15_33 Train loss: 6.641887750902242e-08 at step: 155600 lr 0.00014580000000000002
2024_08_10_15_16_15 Train loss: 6.378163561748806e-06 at step: 156000 lr 0.00014580000000000002
2024_08_10_15_16_56 Train loss: 2.5088721145039017e-07 at step: 156400 lr 0.00014580000000000002
2024_08_10_15_17_38 Train loss: 1.603757127099925e-08 at step: 156800 lr 0.00014580000000000002
2024_08_10_15_18_19 Train loss: 5.346892720248242e-11 at step: 157200 lr 0.00014580000000000002
(Val @ epoch 34) acc: 0.99675; ap: 0.9998094848377357
*************************
2024_08_10_15_19_27
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 99.4
(2 stylegan2 ) acc: 95.4; ap: 99.9
(3 biggan    ) acc: 78.6; ap: 90.1
(4 cyclegan  ) acc: 72.1; ap: 98.6
(5 stargan   ) acc: 93.5; ap: 100.0
(6 gaugan    ) acc: 76.2; ap: 87.4
(7 deepfake  ) acc: 68.0; ap: 75.1
(8 Mean      ) acc: 84.5; ap: 93.8
*************************
2024_08_10_15_23_30
2024_08_10_15_23_42 Train loss: 4.2617325561877806e-06 at step: 157600 lr 0.00014580000000000002
2024_08_10_15_24_24 Train loss: 5.537997935789463e-07 at step: 158000 lr 0.00014580000000000002
2024_08_10_15_25_06 Train loss: 2.982181257493721e-08 at step: 158400 lr 0.00014580000000000002
2024_08_10_15_25_47 Train loss: 2.354392336201272e-06 at step: 158800 lr 0.00014580000000000002
2024_08_10_15_26_29 Train loss: 2.6832776711671613e-07 at step: 159200 lr 0.00014580000000000002
2024_08_10_15_27_10 Train loss: 1.5803604627197265e-13 at step: 159600 lr 0.00014580000000000002
2024_08_10_15_27_52 Train loss: 2.384185791015625e-07 at step: 160000 lr 0.00014580000000000002
2024_08_10_15_28_34 Train loss: 4.912805717793844e-10 at step: 160400 lr 0.00014580000000000002
2024_08_10_15_29_16 Train loss: 8.7786906632914e-09 at step: 160800 lr 0.00014580000000000002
2024_08_10_15_29_58 Train loss: 1.4901328881933296e-07 at step: 161200 lr 0.00014580000000000002
2024_08_10_15_30_40 Train loss: 8.956449448760395e-08 at step: 161600 lr 0.00014580000000000002
2024_08_10_15_31_22 Train loss: 3.814818683167687e-06 at step: 162000 lr 0.00014580000000000002
(Val @ epoch 35) acc: 0.995875; ap: 0.9997834451666376
*************************
2024_08_10_15_31_52
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 94.6; ap: 99.7
(2 stylegan2 ) acc: 97.7; ap: 100.0
(3 biggan    ) acc: 82.7; ap: 89.7
(4 cyclegan  ) acc: 84.3; ap: 98.8
(5 stargan   ) acc: 99.3; ap: 100.0
(6 gaugan    ) acc: 87.1; ap: 88.6
(7 deepfake  ) acc: 66.3; ap: 69.3
(8 Mean      ) acc: 89.0; ap: 93.3
*************************
2024_08_10_15_36_05
2024_08_10_15_36_48 Train loss: 1.4786999599891715e-05 at step: 162400 lr 0.00014580000000000002
2024_08_10_15_37_29 Train loss: 4.699907997507413e-13 at step: 162800 lr 0.00014580000000000002
2024_08_10_15_38_11 Train loss: 1.6796252566564362e-06 at step: 163200 lr 0.00014580000000000002
2024_08_10_15_38_53 Train loss: 3.885752448695712e-07 at step: 163600 lr 0.00014580000000000002
2024_08_10_15_39_34 Train loss: 2.322112209185434e-07 at step: 164000 lr 0.00014580000000000002
2024_08_10_15_40_16 Train loss: 9.328167834610213e-06 at step: 164400 lr 0.00014580000000000002
2024_08_10_15_40_58 Train loss: 6.753572279194486e-08 at step: 164800 lr 0.00014580000000000002
2024_08_10_15_41_39 Train loss: 8.698256692696305e-08 at step: 165200 lr 0.00014580000000000002
2024_08_10_15_42_21 Train loss: 9.12055782009702e-08 at step: 165600 lr 0.00014580000000000002
2024_08_10_15_43_03 Train loss: 0.0006482179160229862 at step: 166000 lr 0.00014580000000000002
2024_08_10_15_43_44 Train loss: 4.187223225926573e-07 at step: 166400 lr 0.00014580000000000002
(Val @ epoch 36) acc: 0.996375; ap: 0.9998748093779863
*************************
2024_08_10_15_44_26
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 99.7
(2 stylegan2 ) acc: 97.2; ap: 100.0
(3 biggan    ) acc: 72.7; ap: 89.4
(4 cyclegan  ) acc: 66.4; ap: 98.6
(5 stargan   ) acc: 78.3; ap: 100.0
(6 gaugan    ) acc: 67.1; ap: 84.8
(7 deepfake  ) acc: 64.3; ap: 70.5
(8 Mean      ) acc: 79.8; ap: 92.9
*************************
2024_08_10_15_48_37
2024_08_10_15_49_09 Train loss: 2.2661738796614372e-07 at step: 166800 lr 0.00014580000000000002
2024_08_10_15_49_51 Train loss: 1.8806897372769527e-08 at step: 167200 lr 0.00014580000000000002
2024_08_10_15_50_33 Train loss: 2.2814319891040213e-05 at step: 167600 lr 0.00014580000000000002
2024_08_10_15_51_14 Train loss: 1.1047992529711337e-06 at step: 168000 lr 0.00014580000000000002
2024_08_10_15_51_56 Train loss: 0.0005458295345306396 at step: 168400 lr 0.00014580000000000002
2024_08_10_15_52_37 Train loss: 0.003215643111616373 at step: 168800 lr 0.00014580000000000002
2024_08_10_15_53_19 Train loss: 6.081235426336207e-08 at step: 169200 lr 0.00014580000000000002
2024_08_10_15_54_01 Train loss: 2.3388686543057702e-08 at step: 169600 lr 0.00014580000000000002
2024_08_10_15_54_42 Train loss: 0.00026892166351899505 at step: 170000 lr 0.00014580000000000002
2024_08_10_15_55_24 Train loss: 1.3058427228429537e-08 at step: 170400 lr 0.00014580000000000002
2024_08_10_15_56_06 Train loss: 1.9932100258301944e-05 at step: 170800 lr 0.00014580000000000002
(Val @ epoch 37) acc: 0.99775; ap: 0.999977080070896
*************************
2024_08_10_15_57_00
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.9; ap: 99.8
(2 stylegan2 ) acc: 96.4; ap: 99.9
(3 biggan    ) acc: 81.9; ap: 88.7
(4 cyclegan  ) acc: 87.9; ap: 98.8
(5 stargan   ) acc: 98.2; ap: 100.0
(6 gaugan    ) acc: 85.4; ap: 83.8
(7 deepfake  ) acc: 72.7; ap: 80.9
(8 Mean      ) acc: 89.7; ap: 94.0
*************************
2024_08_10_16_01_08
2024_08_10_16_01_30 Train loss: 3.391015468423575e-08 at step: 171200 lr 0.00014580000000000002
2024_08_10_16_02_12 Train loss: 3.5814415877410966e-09 at step: 171600 lr 0.00014580000000000002
2024_08_10_16_02_53 Train loss: 3.403425216674805e-05 at step: 172000 lr 0.00014580000000000002
2024_08_10_16_03_35 Train loss: 1.676452754395541e-09 at step: 172400 lr 0.00014580000000000002
2024_08_10_16_04_16 Train loss: 3.522284330870207e-08 at step: 172800 lr 0.00014580000000000002
2024_08_10_16_04_58 Train loss: 1.1920967324385856e-07 at step: 173200 lr 0.00014580000000000002
2024_08_10_16_05_40 Train loss: 1.51206815957039e-08 at step: 173600 lr 0.00014580000000000002
2024_08_10_16_06_21 Train loss: 4.3537465899134986e-07 at step: 174000 lr 0.00014580000000000002
2024_08_10_16_07_03 Train loss: 3.300607204437256e-05 at step: 174400 lr 0.00014580000000000002
2024_08_10_16_07_45 Train loss: 4.621054205955488e-09 at step: 174800 lr 0.00014580000000000002
2024_08_10_16_08_26 Train loss: 3.8743019104003906e-07 at step: 175200 lr 0.00014580000000000002
(Val @ epoch 38) acc: 0.99825; ap: 0.9999975660539492
*************************
2024_08_10_16_09_33
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.5; ap: 99.3
(2 stylegan2 ) acc: 96.5; ap: 100.0
(3 biggan    ) acc: 77.3; ap: 87.4
(4 cyclegan  ) acc: 82.4; ap: 98.8
(5 stargan   ) acc: 99.9; ap: 100.0
(6 gaugan    ) acc: 77.7; ap: 83.6
(7 deepfake  ) acc: 72.6; ap: 77.1
(8 Mean      ) acc: 87.3; ap: 93.3
*************************
2024_08_10_16_13_40
2024_08_10_16_13_51 Train loss: 8.145051744179455e-09 at step: 175600 lr 0.00014580000000000002
2024_08_10_16_14_33 Train loss: 2.861883103832952e-07 at step: 176000 lr 0.00014580000000000002
2024_08_10_16_15_17 Train loss: 0.00428462028503418 at step: 176400 lr 0.00014580000000000002
2024_08_10_16_16_03 Train loss: 5.5789947509765625e-05 at step: 176800 lr 0.00014580000000000002
2024_08_10_16_16_49 Train loss: 8.163823622453492e-06 at step: 177200 lr 0.00014580000000000002
2024_08_10_16_17_35 Train loss: 3.8151625858517946e-08 at step: 177600 lr 0.00014580000000000002
2024_08_10_16_18_18 Train loss: 2.98206082050001e-08 at step: 178000 lr 0.00014580000000000002
2024_08_10_16_19_00 Train loss: 0.0046902671456336975 at step: 178400 lr 0.00014580000000000002
2024_08_10_16_19_41 Train loss: 0.0006766271544620395 at step: 178800 lr 0.00014580000000000002
2024_08_10_16_20_23 Train loss: 0.00041979554225690663 at step: 179200 lr 0.00014580000000000002
2024_08_10_16_21_05 Train loss: 1.1718788162085048e-09 at step: 179600 lr 0.00014580000000000002
2024_08_10_16_21_46 Train loss: 7.331398137466749e-06 at step: 180000 lr 0.00014580000000000002
(Val @ epoch 39) acc: 0.999375; ap: 0.9999988151319692
*************************
2024_08_10_16_22_17
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 91.2; ap: 98.8
(2 stylegan2 ) acc: 95.8; ap: 99.8
(3 biggan    ) acc: 79.5; ap: 87.6
(4 cyclegan  ) acc: 80.7; ap: 98.7
(5 stargan   ) acc: 99.7; ap: 100.0
(6 gaugan    ) acc: 84.0; ap: 82.2
(7 deepfake  ) acc: 62.1; ap: 62.2
(8 Mean      ) acc: 86.6; ap: 91.2
*************************
2024_08_10_16_26_30
2024_08_10_16_27_13 Train loss: 9.808498457886117e-10 at step: 180400 lr 0.00014580000000000002
2024_08_10_16_27_55 Train loss: 4.147062099946197e-06 at step: 180800 lr 0.00014580000000000002
2024_08_10_16_28_36 Train loss: 1.648635361561901e-06 at step: 181200 lr 0.00014580000000000002
2024_08_10_16_29_18 Train loss: 1.9860840083651965e-08 at step: 181600 lr 0.00014580000000000002
2024_08_10_16_29_59 Train loss: 1.002452343357163e-09 at step: 182000 lr 0.00014580000000000002
2024_08_10_16_30_41 Train loss: 2.0873139305876975e-07 at step: 182400 lr 0.00014580000000000002
2024_08_10_16_31_23 Train loss: 1.2045978792230017e-06 at step: 182800 lr 0.00014580000000000002
2024_08_10_16_32_06 Train loss: 6.118572315472193e-11 at step: 183200 lr 0.00014580000000000002
2024_08_10_16_32_51 Train loss: 1.5942043773975456e-07 at step: 183600 lr 0.00014580000000000002
2024_08_10_16_33_35 Train loss: 1.0362767588478761e-10 at step: 184000 lr 0.00014580000000000002
2024_08_10_16_34_19 Train loss: 2.622604597490863e-06 at step: 184400 lr 0.00014580000000000002
saving the model at the end of epoch 40, iters 184541
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_latest.pth
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_40.pth
2024_08_10_16_34_35 changing lr at the end of epoch 40, iters 184541
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 40) acc: 0.99675; ap: 0.9999466493910081
*************************
2024_08_10_16_35_02
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 99.6
(2 stylegan2 ) acc: 95.7; ap: 99.9
(3 biggan    ) acc: 82.3; ap: 90.9
(4 cyclegan  ) acc: 76.5; ap: 98.7
(5 stargan   ) acc: 99.4; ap: 100.0
(6 gaugan    ) acc: 82.4; ap: 86.5
(7 deepfake  ) acc: 70.7; ap: 80.6
(8 Mean      ) acc: 87.4; ap: 94.5
*************************
2024_08_10_16_39_08
2024_08_10_16_39_40 Train loss: 3.503752850519959e-06 at step: 184800 lr 0.00013122000000000003
2024_08_10_16_40_22 Train loss: 8.953237085052024e-08 at step: 185200 lr 0.00013122000000000003
2024_08_10_16_41_03 Train loss: 1.6860693397013904e-10 at step: 185600 lr 0.00013122000000000003
2024_08_10_16_41_45 Train loss: 2.7127091470902087e-06 at step: 186000 lr 0.00013122000000000003
2024_08_10_16_42_27 Train loss: 4.13316385561302e-08 at step: 186400 lr 0.00013122000000000003
2024_08_10_16_43_09 Train loss: 4.4956857891520485e-05 at step: 186800 lr 0.00013122000000000003
2024_08_10_16_43_51 Train loss: 1.7583483895577956e-06 at step: 187200 lr 0.00013122000000000003
2024_08_10_16_44_32 Train loss: 1.3756057404279431e-10 at step: 187600 lr 0.00013122000000000003
2024_08_10_16_45_14 Train loss: 1.0312368736720146e-09 at step: 188000 lr 0.00013122000000000003
2024_08_10_16_45_56 Train loss: 4.376553988549858e-06 at step: 188400 lr 0.00013122000000000003
2024_08_10_16_46_37 Train loss: 4.3778464187127497e-10 at step: 188800 lr 0.00013122000000000003
(Val @ epoch 41) acc: 0.99625; ap: 0.9998303561192133
*************************
2024_08_10_16_47_31
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 96.0; ap: 99.4
(2 stylegan2 ) acc: 98.7; ap: 100.0
(3 biggan    ) acc: 75.8; ap: 88.3
(4 cyclegan  ) acc: 69.1; ap: 97.9
(5 stargan   ) acc: 98.2; ap: 99.8
(6 gaugan    ) acc: 71.6; ap: 84.5
(7 deepfake  ) acc: 68.3; ap: 78.5
(8 Mean      ) acc: 84.7; ap: 93.5
*************************
2024_08_10_16_51_37
2024_08_10_16_51_58 Train loss: 1.2156766704407573e-09 at step: 189200 lr 0.00013122000000000003
2024_08_10_16_52_39 Train loss: 1.4489716718135703e-11 at step: 189600 lr 0.00013122000000000003
2024_08_10_16_53_21 Train loss: 8.899146068490349e-13 at step: 190000 lr 0.00013122000000000003
2024_08_10_16_54_03 Train loss: 2.833447751982021e-06 at step: 190400 lr 0.00013122000000000003
2024_08_10_16_54_44 Train loss: 1.2462997744933091e-08 at step: 190800 lr 0.00013122000000000003
2024_08_10_16_55_26 Train loss: 2.7685896952789335e-07 at step: 191200 lr 0.00013122000000000003
2024_08_10_16_56_08 Train loss: 1.3175331048387307e-07 at step: 191600 lr 0.00013122000000000003
2024_08_10_16_56_49 Train loss: 1.9621403113401215e-10 at step: 192000 lr 0.00013122000000000003
2024_08_10_16_57_31 Train loss: 1.0883989022547613e-12 at step: 192400 lr 0.00013122000000000003
2024_08_10_16_58_12 Train loss: 8.486236424687377e-07 at step: 192800 lr 0.00013122000000000003
2024_08_10_16_58_54 Train loss: 5.644497491630318e-07 at step: 193200 lr 0.00013122000000000003
(Val @ epoch 42) acc: 0.994; ap: 0.9995470506924062
*************************
2024_08_10_16_59_57
(0 progan    ) acc: 99.6; ap: 100.0
(1 stylegan  ) acc: 94.3; ap: 99.6
(2 stylegan2 ) acc: 94.9; ap: 99.9
(3 biggan    ) acc: 81.2; ap: 88.5
(4 cyclegan  ) acc: 79.6; ap: 98.7
(5 stargan   ) acc: 99.7; ap: 100.0
(6 gaugan    ) acc: 82.7; ap: 86.8
(7 deepfake  ) acc: 74.3; ap: 81.9
(8 Mean      ) acc: 88.3; ap: 94.4
*************************
2024_08_10_17_04_07
2024_08_10_17_04_19 Train loss: 1.6689300537109375e-06 at step: 193600 lr 0.00013122000000000003
2024_08_10_17_05_01 Train loss: 2.283161592892328e-18 at step: 194000 lr 0.00013122000000000003
2024_08_10_17_05_42 Train loss: 1.1721139466999708e-12 at step: 194400 lr 0.00013122000000000003
2024_08_10_17_06_24 Train loss: 1.529945450329251e-07 at step: 194800 lr 0.00013122000000000003
2024_08_10_17_07_06 Train loss: 2.4714546498216805e-07 at step: 195200 lr 0.00013122000000000003
2024_08_10_17_07_47 Train loss: 3.233034817640146e-08 at step: 195600 lr 0.00013122000000000003
2024_08_10_17_08_29 Train loss: 2.1387172637332696e-06 at step: 196000 lr 0.00013122000000000003
2024_08_10_17_09_11 Train loss: 5.253960466689023e-07 at step: 196400 lr 0.00013122000000000003
2024_08_10_17_09_52 Train loss: 6.3238912844099104e-06 at step: 196800 lr 0.00013122000000000003
2024_08_10_17_10_35 Train loss: 3.257263969658197e-08 at step: 197200 lr 0.00013122000000000003
2024_08_10_17_11_17 Train loss: 2.984544877904227e-08 at step: 197600 lr 0.00013122000000000003
2024_08_10_17_11_59 Train loss: 1.803864790872467e-07 at step: 198000 lr 0.00013122000000000003
(Val @ epoch 43) acc: 0.9985; ap: 0.9999879026506245
*************************
2024_08_10_17_12_30
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.1; ap: 99.8
(2 stylegan2 ) acc: 96.1; ap: 99.9
(3 biggan    ) acc: 83.3; ap: 89.7
(4 cyclegan  ) acc: 79.9; ap: 98.9
(5 stargan   ) acc: 99.7; ap: 100.0
(6 gaugan    ) acc: 82.9; ap: 85.4
(7 deepfake  ) acc: 67.8; ap: 72.8
(8 Mean      ) acc: 88.0; ap: 93.3
*************************
2024_08_10_17_16_34
2024_08_10_17_17_16 Train loss: 1.4296158212800947e-07 at step: 198400 lr 0.00013122000000000003
2024_08_10_17_17_58 Train loss: 2.9878112428605164e-08 at step: 198800 lr 0.00013122000000000003
2024_08_10_17_18_39 Train loss: 3.625600470888446e-12 at step: 199200 lr 0.00013122000000000003
2024_08_10_17_19_21 Train loss: 2.6836676170205465e-07 at step: 199600 lr 0.00013122000000000003
2024_08_10_17_20_03 Train loss: 2.856572507425881e-07 at step: 200000 lr 0.00013122000000000003
2024_08_10_17_20_44 Train loss: 5.678551517895869e-10 at step: 200400 lr 0.00013122000000000003
2024_08_10_17_21_26 Train loss: 4.443899404276558e-14 at step: 200800 lr 0.00013122000000000003
2024_08_10_17_22_08 Train loss: 1.4318021612780285e-06 at step: 201200 lr 0.00013122000000000003
2024_08_10_17_22_50 Train loss: 1.7892064363422833e-07 at step: 201600 lr 0.00013122000000000003
2024_08_10_17_23_32 Train loss: 9.731736838602956e-08 at step: 202000 lr 0.00013122000000000003
2024_08_10_17_24_14 Train loss: 3.129243850708008e-06 at step: 202400 lr 0.00013122000000000003
(Val @ epoch 44) acc: 0.996625; ap: 0.9999167732792572
*************************
2024_08_10_17_24_55
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 93.3; ap: 99.7
(2 stylegan2 ) acc: 98.0; ap: 100.0
(3 biggan    ) acc: 78.8; ap: 90.0
(4 cyclegan  ) acc: 73.3; ap: 98.6
(5 stargan   ) acc: 97.0; ap: 100.0
(6 gaugan    ) acc: 75.4; ap: 84.7
(7 deepfake  ) acc: 68.8; ap: 70.3
(8 Mean      ) acc: 85.5; ap: 92.9
*************************
2024_08_10_17_28_56
2024_08_10_17_29_27 Train loss: 2.9802382073285116e-07 at step: 202800 lr 0.00013122000000000003
2024_08_10_17_30_09 Train loss: 2.148752537323162e-05 at step: 203200 lr 0.00013122000000000003
2024_08_10_17_30_50 Train loss: 1.8680795799141947e-10 at step: 203600 lr 0.00013122000000000003
2024_08_10_17_31_32 Train loss: 1.52758431681832e-07 at step: 204000 lr 0.00013122000000000003
2024_08_10_17_32_14 Train loss: 1.6093395061034244e-06 at step: 204400 lr 0.00013122000000000003
2024_08_10_17_32_55 Train loss: 2.682209014892578e-07 at step: 204800 lr 0.00013122000000000003
2024_08_10_17_33_37 Train loss: 5.167808194528334e-05 at step: 205200 lr 0.00013122000000000003
2024_08_10_17_34_19 Train loss: 3.65161595405894e-10 at step: 205600 lr 0.00013122000000000003
2024_08_10_17_35_00 Train loss: 6.68775657075571e-09 at step: 206000 lr 0.00013122000000000003
2024_08_10_17_35_42 Train loss: 7.283687591552734e-05 at step: 206400 lr 0.00013122000000000003
2024_08_10_17_36_24 Train loss: 5.347556286672273e-10 at step: 206800 lr 0.00013122000000000003
(Val @ epoch 45) acc: 0.9975; ap: 0.9999668447362925
*************************
2024_08_10_17_37_15
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 96.0; ap: 99.8
(2 stylegan2 ) acc: 97.8; ap: 100.0
(3 biggan    ) acc: 84.6; ap: 89.3
(4 cyclegan  ) acc: 87.9; ap: 98.8
(5 stargan   ) acc: 98.6; ap: 99.9
(6 gaugan    ) acc: 84.8; ap: 87.5
(7 deepfake  ) acc: 66.6; ap: 70.7
(8 Mean      ) acc: 89.5; ap: 93.2
*************************
2024_08_10_17_41_21
2024_08_10_17_41_42 Train loss: 5.069360327070171e-07 at step: 207200 lr 0.00013122000000000003
2024_08_10_17_42_23 Train loss: 3.120019087532455e-08 at step: 207600 lr 0.00013122000000000003
2024_08_10_17_43_05 Train loss: 3.127570979977179e-10 at step: 208000 lr 0.00013122000000000003
2024_08_10_17_43_46 Train loss: 1.0035419109044597e-06 at step: 208400 lr 0.00013122000000000003
2024_08_10_17_44_28 Train loss: 4.828326837014174e-06 at step: 208800 lr 0.00013122000000000003
2024_08_10_17_45_10 Train loss: 2.4521664272469934e-06 at step: 209200 lr 0.00013122000000000003
2024_08_10_17_45_52 Train loss: 2.4369086531805806e-05 at step: 209600 lr 0.00013122000000000003
2024_08_10_17_46_33 Train loss: 1.3113128716213396e-06 at step: 210000 lr 0.00013122000000000003
2024_08_10_17_47_15 Train loss: 1.0298888613669357e-13 at step: 210400 lr 0.00013122000000000003
2024_08_10_17_47_56 Train loss: 5.1412889661150984e-06 at step: 210800 lr 0.00013122000000000003
2024_08_10_17_48_38 Train loss: 7.907339494295229e-08 at step: 211200 lr 0.00013122000000000003
(Val @ epoch 46) acc: 0.992875; ap: 0.999958506325209
*************************
2024_08_10_17_49_41
(0 progan    ) acc: 99.3; ap: 100.0
(1 stylegan  ) acc: 95.1; ap: 99.0
(2 stylegan2 ) acc: 99.3; ap: 100.0
(3 biggan    ) acc: 76.2; ap: 86.0
(4 cyclegan  ) acc: 71.8; ap: 96.9
(5 stargan   ) acc: 81.7; ap: 99.8
(6 gaugan    ) acc: 71.9; ap: 77.8
(7 deepfake  ) acc: 73.1; ap: 82.9
(8 Mean      ) acc: 83.6; ap: 92.8
*************************
2024_08_10_17_53_47
2024_08_10_17_53_57 Train loss: 1.2966719364904122e-10 at step: 211600 lr 0.00013122000000000003
2024_08_10_17_54_39 Train loss: 3.1231994768177174e-08 at step: 212000 lr 0.00013122000000000003
2024_08_10_17_55_20 Train loss: 1.490940064741153e-07 at step: 212400 lr 0.00013122000000000003
2024_08_10_17_56_02 Train loss: 4.201822001448363e-09 at step: 212800 lr 0.00013122000000000003
2024_08_10_17_56_44 Train loss: 7.950154667923925e-07 at step: 213200 lr 0.00013122000000000003
2024_08_10_17_57_25 Train loss: 2.2948568584979512e-05 at step: 213600 lr 0.00013122000000000003
2024_08_10_17_58_07 Train loss: 4.989331969795785e-08 at step: 214000 lr 0.00013122000000000003
2024_08_10_17_58_49 Train loss: 5.914004841400811e-10 at step: 214400 lr 0.00013122000000000003
2024_08_10_17_59_30 Train loss: 5.155801773071289e-06 at step: 214800 lr 0.00013122000000000003
2024_08_10_18_00_12 Train loss: 1.9376577470623605e-16 at step: 215200 lr 0.00013122000000000003
2024_08_10_18_00_53 Train loss: 3.3682079477945215e-13 at step: 215600 lr 0.00013122000000000003
2024_08_10_18_01_35 Train loss: 6.606799995978463e-09 at step: 216000 lr 0.00013122000000000003
(Val @ epoch 47) acc: 0.995625; ap: 0.9999700608481692
*************************
2024_08_10_18_02_07
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.1; ap: 99.8
(2 stylegan2 ) acc: 93.7; ap: 100.0
(3 biggan    ) acc: 77.0; ap: 91.2
(4 cyclegan  ) acc: 67.3; ap: 98.8
(5 stargan   ) acc: 88.2; ap: 100.0
(6 gaugan    ) acc: 69.8; ap: 84.7
(7 deepfake  ) acc: 68.3; ap: 76.0
(8 Mean      ) acc: 82.0; ap: 93.8
*************************
2024_08_10_18_06_13
2024_08_10_18_06_55 Train loss: 6.965991872220911e-08 at step: 216400 lr 0.00013122000000000003
2024_08_10_18_07_37 Train loss: 1.1633461773571442e-22 at step: 216800 lr 0.00013122000000000003
2024_08_10_18_08_18 Train loss: 7.721137618155183e-11 at step: 217200 lr 0.00013122000000000003
2024_08_10_18_09_00 Train loss: 0.00040391087532043457 at step: 217600 lr 0.00013122000000000003
2024_08_10_18_09_42 Train loss: 0.00011390447616577148 at step: 218000 lr 0.00013122000000000003
2024_08_10_18_10_23 Train loss: 0.007451202720403671 at step: 218400 lr 0.00013122000000000003
2024_08_10_18_11_05 Train loss: 8.940696716308594e-08 at step: 218800 lr 0.00013122000000000003
2024_08_10_18_11_47 Train loss: 2.785505444080627e-07 at step: 219200 lr 0.00013122000000000003
2024_08_10_18_12_28 Train loss: 1.6085529574993984e-09 at step: 219600 lr 0.00013122000000000003
2024_08_10_18_13_10 Train loss: 1.4383059578904067e-06 at step: 220000 lr 0.00013122000000000003
2024_08_10_18_13_51 Train loss: 7.276404403455672e-07 at step: 220400 lr 0.00013122000000000003
(Val @ epoch 48) acc: 0.997375; ap: 0.9998665386568031
*************************
2024_08_10_18_14_33
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.1; ap: 99.1
(2 stylegan2 ) acc: 95.6; ap: 99.9
(3 biggan    ) acc: 76.6; ap: 85.5
(4 cyclegan  ) acc: 75.9; ap: 98.6
(5 stargan   ) acc: 74.7; ap: 99.8
(6 gaugan    ) acc: 74.4; ap: 78.7
(7 deepfake  ) acc: 64.2; ap: 65.4
(8 Mean      ) acc: 81.5; ap: 90.9
*************************
2024_08_10_18_18_39
2024_08_10_18_19_10 Train loss: 1.0184005532209994e-06 at step: 220800 lr 0.00013122000000000003
2024_08_10_18_19_51 Train loss: 9.483722465120081e-07 at step: 221200 lr 0.00013122000000000003
2024_08_10_18_20_33 Train loss: 1.7832610055279474e-08 at step: 221600 lr 0.00013122000000000003
2024_08_10_18_21_15 Train loss: 4.554223664854362e-10 at step: 222000 lr 0.00013122000000000003
2024_08_10_18_21_56 Train loss: 5.430554779195518e-07 at step: 222400 lr 0.00013122000000000003
2024_08_10_18_22_39 Train loss: 8.226570571423508e-07 at step: 222800 lr 0.00013122000000000003
2024_08_10_18_23_20 Train loss: 6.27404617148386e-08 at step: 223200 lr 0.00013122000000000003
2024_08_10_18_24_02 Train loss: 2.1564676178651224e-14 at step: 223600 lr 0.00013122000000000003
2024_08_10_18_24_44 Train loss: 4.33946656386297e-09 at step: 224000 lr 0.00013122000000000003
2024_08_10_18_25_25 Train loss: 1.2730211551570392e-08 at step: 224400 lr 0.00013122000000000003
2024_08_10_18_26_07 Train loss: 1.599005190655589e-05 at step: 224800 lr 0.00013122000000000003
(Val @ epoch 49) acc: 0.99625; ap: 0.9999063210629745
*************************
2024_08_10_18_27_00
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.9; ap: 99.9
(2 stylegan2 ) acc: 93.2; ap: 100.0
(3 biggan    ) acc: 80.5; ap: 90.6
(4 cyclegan  ) acc: 79.0; ap: 99.2
(5 stargan   ) acc: 89.3; ap: 100.0
(6 gaugan    ) acc: 83.5; ap: 85.7
(7 deepfake  ) acc: 58.6; ap: 67.3
(8 Mean      ) acc: 84.5; ap: 92.8
*************************
2024_08_10_18_31_07
2024_08_10_18_31_28 Train loss: 5.234063564785174e-07 at step: 225200 lr 0.00013122000000000003
2024_08_10_18_32_10 Train loss: 2.4082536312364233e-11 at step: 225600 lr 0.00013122000000000003
2024_08_10_18_32_52 Train loss: 2.682211857063521e-07 at step: 226000 lr 0.00013122000000000003
2024_08_10_18_33_33 Train loss: 8.43769498715119e-11 at step: 226400 lr 0.00013122000000000003
2024_08_10_18_34_15 Train loss: 3.040224328287877e-05 at step: 226800 lr 0.00013122000000000003
2024_08_10_18_34_57 Train loss: 3.023310313210459e-08 at step: 227200 lr 0.00013122000000000003
2024_08_10_18_35_38 Train loss: 3.4262050661870713e-12 at step: 227600 lr 0.00013122000000000003
2024_08_10_18_36_20 Train loss: 2.0073321477243056e-11 at step: 228000 lr 0.00013122000000000003
2024_08_10_18_37_01 Train loss: 2.980241831096464e-08 at step: 228400 lr 0.00013122000000000003
2024_08_10_18_37_43 Train loss: 0.00014150881906971335 at step: 228800 lr 0.00013122000000000003
2024_08_10_18_38_25 Train loss: 8.941069751244868e-08 at step: 229200 lr 0.00013122000000000003
2024_08_10_18_39_02 changing lr at the end of epoch 50, iters 229551
*************************
Changing lr from 0.00013122000000000003 to 0.00011809800000000003
*************************
(Val @ epoch 50) acc: 0.995125; ap: 0.9999143780968929
*************************
2024_08_10_18_39_27
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 94.6; ap: 99.9
(2 stylegan2 ) acc: 97.2; ap: 100.0
(3 biggan    ) acc: 83.6; ap: 89.8
(4 cyclegan  ) acc: 87.0; ap: 98.9
(5 stargan   ) acc: 97.4; ap: 100.0
(6 gaugan    ) acc: 87.1; ap: 86.3
(7 deepfake  ) acc: 57.1; ap: 76.8
(8 Mean      ) acc: 88.0; ap: 93.9
*************************
2024_08_10_18_43_39
2024_08_10_18_43_49 Train loss: 1.225801460336129e-09 at step: 229600 lr 0.00011809800000000003
2024_08_10_18_44_31 Train loss: 3.461442332763909e-08 at step: 230000 lr 0.00011809800000000003
2024_08_10_18_45_13 Train loss: 1.2815172567570698e-06 at step: 230400 lr 0.00011809800000000003
2024_08_10_18_45_55 Train loss: 1.1324304161064447e-08 at step: 230800 lr 0.00011809800000000003
2024_08_10_18_46_37 Train loss: 1.5206748003038228e-06 at step: 231200 lr 0.00011809800000000003
2024_08_10_18_47_19 Train loss: 1.5573575415372964e-11 at step: 231600 lr 0.00011809800000000003
2024_08_10_18_48_01 Train loss: 4.388460183690768e-06 at step: 232000 lr 0.00011809800000000003
2024_08_10_18_48_42 Train loss: 2.980336688551688e-08 at step: 232400 lr 0.00011809800000000003
2024_08_10_18_49_24 Train loss: 6.038181022890998e-13 at step: 232800 lr 0.00011809800000000003
2024_08_10_18_50_06 Train loss: 3.239771645402456e-11 at step: 233200 lr 0.00011809800000000003
2024_08_10_18_50_47 Train loss: 0.0005313577130436897 at step: 233600 lr 0.00011809800000000003
2024_08_10_18_51_29 Train loss: 9.546073442834313e-07 at step: 234000 lr 0.00011809800000000003
(Val @ epoch 51) acc: 0.995375; ap: 0.9997578268784977
*************************
2024_08_10_18_52_04
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 96.8; ap: 99.8
(2 stylegan2 ) acc: 99.6; ap: 100.0
(3 biggan    ) acc: 81.5; ap: 86.5
(4 cyclegan  ) acc: 84.3; ap: 98.5
(5 stargan   ) acc: 99.8; ap: 100.0
(6 gaugan    ) acc: 80.2; ap: 79.4
(7 deepfake  ) acc: 62.1; ap: 67.5
(8 Mean      ) acc: 88.0; ap: 91.5
*************************
2024_08_10_18_56_10
2024_08_10_18_56_51 Train loss: 8.946859963998577e-08 at step: 234400 lr 0.00011809800000000003
2024_08_10_18_57_33 Train loss: 6.636499165324494e-05 at step: 234800 lr 0.00011809800000000003
2024_08_10_18_58_14 Train loss: 1.2039591316396656e-16 at step: 235200 lr 0.00011809800000000003
2024_08_10_18_58_56 Train loss: 2.528465302020777e-06 at step: 235600 lr 0.00011809800000000003
2024_08_10_18_59_38 Train loss: 4.733421432590035e-10 at step: 236000 lr 0.00011809800000000003
2024_08_10_19_00_19 Train loss: 6.624059634141588e-10 at step: 236400 lr 0.00011809800000000003
2024_08_10_19_01_01 Train loss: 5.941033123235684e-07 at step: 236800 lr 0.00011809800000000003
2024_08_10_19_01_42 Train loss: 5.78286858399224e-07 at step: 237200 lr 0.00011809800000000003
2024_08_10_19_02_25 Train loss: 2.6341820102970814e-06 at step: 237600 lr 0.00011809800000000003
2024_08_10_19_03_08 Train loss: 1.8535060064550635e-07 at step: 238000 lr 0.00011809800000000003
2024_08_10_19_03_53 Train loss: 3.6571909944882464e-09 at step: 238400 lr 0.00011809800000000003
(Val @ epoch 52) acc: 0.99825; ap: 0.9999014569360403
*************************
2024_08_10_19_04_36
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 96.8; ap: 99.7
(2 stylegan2 ) acc: 98.7; ap: 100.0
(3 biggan    ) acc: 77.6; ap: 87.7
(4 cyclegan  ) acc: 75.5; ap: 98.7
(5 stargan   ) acc: 95.0; ap: 100.0
(6 gaugan    ) acc: 77.3; ap: 82.8
(7 deepfake  ) acc: 70.6; ap: 73.8
(8 Mean      ) acc: 86.4; ap: 92.8
*************************
2024_08_10_19_08_44
2024_08_10_19_09_14 Train loss: 5.240469391765146e-09 at step: 238800 lr 0.00011809800000000003
2024_08_10_19_09_56 Train loss: 2.9915955934711747e-08 at step: 239200 lr 0.00011809800000000003
2024_08_10_19_10_38 Train loss: 1.529788960397127e-06 at step: 239600 lr 0.00011809800000000003
2024_08_10_19_11_19 Train loss: 1.8369148619967746e-06 at step: 240000 lr 0.00011809800000000003
2024_08_10_19_12_03 Train loss: 2.9359317304774105e-14 at step: 240400 lr 0.00011809800000000003
2024_08_10_19_12_45 Train loss: 1.8298625946044922e-05 at step: 240800 lr 0.00011809800000000003
2024_08_10_19_13_27 Train loss: 5.662441253662109e-07 at step: 241200 lr 0.00011809800000000003
2024_08_10_19_14_09 Train loss: 7.587753003690523e-08 at step: 241600 lr 0.00011809800000000003
2024_08_10_19_14_50 Train loss: 8.94214693403228e-08 at step: 242000 lr 0.00011809800000000003
2024_08_10_19_15_32 Train loss: 9.984698623455301e-10 at step: 242400 lr 0.00011809800000000003
2024_08_10_19_16_14 Train loss: 1.055117432769044e-14 at step: 242800 lr 0.00011809800000000003
(Val @ epoch 53) acc: 0.998; ap: 0.9999526061636838
*************************
2024_08_10_19_17_07
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.7; ap: 98.8
(2 stylegan2 ) acc: 96.4; ap: 99.9
(3 biggan    ) acc: 73.9; ap: 87.8
(4 cyclegan  ) acc: 73.0; ap: 98.9
(5 stargan   ) acc: 87.2; ap: 100.0
(6 gaugan    ) acc: 72.4; ap: 83.1
(7 deepfake  ) acc: 68.0; ap: 71.4
(8 Mean      ) acc: 82.9; ap: 92.5
*************************
2024_08_10_19_21_12
2024_08_10_19_21_33 Train loss: 8.561709927991639e-15 at step: 243200 lr 0.00011809800000000003
2024_08_10_19_22_17 Train loss: 1.6237275413111263e-11 at step: 243600 lr 0.00011809800000000003
2024_08_10_19_23_01 Train loss: 4.1861206168114506e-10 at step: 244000 lr 0.00011809800000000003
2024_08_10_19_23_43 Train loss: 1.1538613814421339e-10 at step: 244400 lr 0.00011809800000000003
2024_08_10_19_24_25 Train loss: 0.00035470008151605725 at step: 244800 lr 0.00011809800000000003
2024_08_10_19_25_06 Train loss: 2.38720012646354e-08 at step: 245200 lr 0.00011809800000000003
2024_08_10_19_25_48 Train loss: 4.470351484542334e-07 at step: 245600 lr 0.00011809800000000003
2024_08_10_19_26_30 Train loss: 4.986013664165512e-05 at step: 246000 lr 0.00011809800000000003
2024_08_10_19_27_11 Train loss: 1.967816361247987e-11 at step: 246400 lr 0.00011809800000000003
2024_08_10_19_27_53 Train loss: 2.364950660194154e-06 at step: 246800 lr 0.00011809800000000003
2024_08_10_19_28_35 Train loss: 5.4901164503462496e-08 at step: 247200 lr 0.00011809800000000003
(Val @ epoch 54) acc: 0.997125; ap: 0.9998237559084856
*************************
2024_08_10_19_29_39
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.0; ap: 99.7
(2 stylegan2 ) acc: 93.9; ap: 99.9
(3 biggan    ) acc: 84.5; ap: 91.6
(4 cyclegan  ) acc: 75.2; ap: 98.4
(5 stargan   ) acc: 92.7; ap: 99.6
(6 gaugan    ) acc: 85.7; ap: 88.8
(7 deepfake  ) acc: 63.6; ap: 63.8
(8 Mean      ) acc: 86.2; ap: 92.7
*************************
2024_08_10_19_33_44
2024_08_10_19_33_53 Train loss: 9.958706641555182e-07 at step: 247600 lr 0.00011809800000000003
2024_08_10_19_34_34 Train loss: 6.427480002457742e-06 at step: 248000 lr 0.00011809800000000003
2024_08_10_19_35_16 Train loss: 1.4907404022324044e-07 at step: 248400 lr 0.00011809800000000003
2024_08_10_19_35_58 Train loss: 1.0238178447252722e-06 at step: 248800 lr 0.00011809800000000003
2024_08_10_19_36_39 Train loss: 5.961160809420107e-08 at step: 249200 lr 0.00011809800000000003
2024_08_10_19_37_21 Train loss: 1.3029684886589621e-08 at step: 249600 lr 0.00011809800000000003
2024_08_10_19_38_04 Train loss: 5.9848827049791e-07 at step: 250000 lr 0.00011809800000000003
2024_08_10_19_38_47 Train loss: 1.3432508577659874e-11 at step: 250400 lr 0.00011809800000000003
2024_08_10_19_39_32 Train loss: 4.244327938694899e-15 at step: 250800 lr 0.00011809800000000003
2024_08_10_19_40_15 Train loss: 1.358323392999794e-12 at step: 251200 lr 0.00011809800000000003
2024_08_10_19_40_58 Train loss: 1.4175880025391052e-08 at step: 251600 lr 0.00011809800000000003
2024_08_10_19_41_40 Train loss: 8.828708928376727e-07 at step: 252000 lr 0.00011809800000000003
(Val @ epoch 55) acc: 0.9965; ap: 0.9999409379846621
*************************
2024_08_10_19_42_14
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.3; ap: 99.7
(2 stylegan2 ) acc: 96.8; ap: 100.0
(3 biggan    ) acc: 78.8; ap: 89.1
(4 cyclegan  ) acc: 75.7; ap: 98.2
(5 stargan   ) acc: 98.5; ap: 100.0
(6 gaugan    ) acc: 81.5; ap: 85.2
(7 deepfake  ) acc: 68.5; ap: 71.3
(8 Mean      ) acc: 86.7; ap: 92.9
*************************
2024_08_10_19_46_17
2024_08_10_19_46_57 Train loss: 3.063681288040243e-05 at step: 252400 lr 0.00011809800000000003
2024_08_10_19_47_39 Train loss: 8.665325168522031e-08 at step: 252800 lr 0.00011809800000000003
2024_08_10_19_48_20 Train loss: 1.862461126620439e-14 at step: 253200 lr 0.00011809800000000003
2024_08_10_19_49_02 Train loss: 2.172080399986953e-07 at step: 253600 lr 0.00011809800000000003
2024_08_10_19_49_43 Train loss: 2.9808296631017583e-07 at step: 254000 lr 0.00011809800000000003
2024_08_10_19_50_25 Train loss: 3.4496188163757324e-05 at step: 254400 lr 0.00011809800000000003
2024_08_10_19_51_07 Train loss: 5.136823041582139e-13 at step: 254800 lr 0.00011809800000000003
2024_08_10_19_51_48 Train loss: 7.68521335459127e-09 at step: 255200 lr 0.00011809800000000003
2024_08_10_19_52_30 Train loss: 2.6822092991096724e-07 at step: 255600 lr 0.00011809800000000003
2024_08_10_19_53_12 Train loss: 9.851157665252686e-05 at step: 256000 lr 0.00011809800000000003
2024_08_10_19_53_53 Train loss: 1.2730486803613772e-11 at step: 256400 lr 0.00011809800000000003
(Val @ epoch 56) acc: 0.9945; ap: 0.999568951625838
*************************
2024_08_10_19_54_36
(0 progan    ) acc: 99.6; ap: 100.0
(1 stylegan  ) acc: 93.1; ap: 99.9
(2 stylegan2 ) acc: 93.4; ap: 99.9
(3 biggan    ) acc: 83.6; ap: 90.8
(4 cyclegan  ) acc: 73.7; ap: 98.0
(5 stargan   ) acc: 99.8; ap: 100.0
(6 gaugan    ) acc: 80.6; ap: 83.8
(7 deepfake  ) acc: 57.2; ap: 70.5
(8 Mean      ) acc: 85.1; ap: 92.9
*************************
2024_08_10_19_58_41
2024_08_10_19_59_11 Train loss: 5.132254045747686e-06 at step: 256800 lr 0.00011809800000000003
2024_08_10_19_59_53 Train loss: 1.0718043119695153e-10 at step: 257200 lr 0.00011809800000000003
2024_08_10_20_00_35 Train loss: 5.808775039016482e-19 at step: 257600 lr 0.00011809800000000003
2024_08_10_20_01_16 Train loss: 1.7881393432617188e-07 at step: 258000 lr 0.00011809800000000003
2024_08_10_20_01_58 Train loss: 2.0217685858803236e-10 at step: 258400 lr 0.00011809800000000003
2024_08_10_20_02_40 Train loss: 5.963270410802579e-08 at step: 258800 lr 0.00011809800000000003
2024_08_10_20_03_21 Train loss: 7.749181157867824e-11 at step: 259200 lr 0.00011809800000000003
2024_08_10_20_04_03 Train loss: 3.331260961525473e-11 at step: 259600 lr 0.00011809800000000003
2024_08_10_20_04_44 Train loss: 0.0004903714871034026 at step: 260000 lr 0.00011809800000000003
2024_08_10_20_05_26 Train loss: 1.6868919455736986e-11 at step: 260400 lr 0.00011809800000000003
2024_08_10_20_06_08 Train loss: 1.0676639433659929e-10 at step: 260800 lr 0.00011809800000000003
(Val @ epoch 57) acc: 0.996625; ap: 0.999934196072835
*************************
2024_08_10_20_07_01
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.8; ap: 99.9
(2 stylegan2 ) acc: 95.2; ap: 99.9
(3 biggan    ) acc: 83.7; ap: 87.7
(4 cyclegan  ) acc: 84.4; ap: 99.1
(5 stargan   ) acc: 99.1; ap: 100.0
(6 gaugan    ) acc: 80.2; ap: 77.5
(7 deepfake  ) acc: 63.3; ap: 79.2
(8 Mean      ) acc: 87.6; ap: 92.9
*************************
2024_08_10_20_11_05
2024_08_10_20_11_24 Train loss: 7.162988185882568e-05 at step: 261200 lr 0.00011809800000000003
2024_08_10_20_12_06 Train loss: 1.4871588064124808e-05 at step: 261600 lr 0.00011809800000000003
2024_08_10_20_12_48 Train loss: 5.075638694063134e-12 at step: 262000 lr 0.00011809800000000003
2024_08_10_20_13_32 Train loss: 9.50407308408785e-10 at step: 262400 lr 0.00011809800000000003
2024_08_10_20_14_17 Train loss: 3.0308179702842608e-05 at step: 262800 lr 0.00011809800000000003
2024_08_10_20_15_00 Train loss: 4.231637609763972e-11 at step: 263200 lr 0.00011809800000000003
2024_08_10_20_15_45 Train loss: 1.1987503967247903e-07 at step: 263600 lr 0.00011809800000000003
2024_08_10_20_16_27 Train loss: 1.7170100410912603e-11 at step: 264000 lr 0.00011809800000000003
2024_08_10_20_17_09 Train loss: 2.695010534181641e-11 at step: 264400 lr 0.00011809800000000003
2024_08_10_20_17_51 Train loss: 2.9802828294123174e-07 at step: 264800 lr 0.00011809800000000003
2024_08_10_20_18_32 Train loss: 7.462528373025634e-09 at step: 265200 lr 0.00011809800000000003
(Val @ epoch 58) acc: 0.996875; ap: 0.9999333824985381
*************************
2024_08_10_20_19_36
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 95.2; ap: 99.8
(2 stylegan2 ) acc: 95.9; ap: 99.9
(3 biggan    ) acc: 80.9; ap: 91.6
(4 cyclegan  ) acc: 70.1; ap: 98.4
(5 stargan   ) acc: 98.4; ap: 100.0
(6 gaugan    ) acc: 79.0; ap: 88.9
(7 deepfake  ) acc: 74.1; ap: 81.6
(8 Mean      ) acc: 86.7; ap: 95.0
*************************
2024_08_10_20_23_42
2024_08_10_20_23_51 Train loss: 2.4124877651132692e-09 at step: 265600 lr 0.00011809800000000003
2024_08_10_20_24_32 Train loss: 8.67400335585744e-12 at step: 266000 lr 0.00011809800000000003
2024_08_10_20_25_14 Train loss: 9.703636897029355e-05 at step: 266400 lr 0.00011809800000000003
2024_08_10_20_25_56 Train loss: 5.681543484570284e-07 at step: 266800 lr 0.00011809800000000003
2024_08_10_20_26_37 Train loss: 1.5087749105635595e-10 at step: 267200 lr 0.00011809800000000003
2024_08_10_20_27_19 Train loss: 9.779987486524533e-09 at step: 267600 lr 0.00011809800000000003
2024_08_10_20_28_01 Train loss: 3.294023898092746e-08 at step: 268000 lr 0.00011809800000000003
2024_08_10_20_28_42 Train loss: 1.2040139154123608e-05 at step: 268400 lr 0.00011809800000000003
2024_08_10_20_29_24 Train loss: 7.450580596923828e-07 at step: 268800 lr 0.00011809800000000003
2024_08_10_20_30_06 Train loss: 1.71959418366896e-05 at step: 269200 lr 0.00011809800000000003
2024_08_10_20_30_49 Train loss: 1.4075194343377007e-08 at step: 269600 lr 0.00011809800000000003
2024_08_10_20_31_33 Train loss: 2.041608468061895e-06 at step: 270000 lr 0.00011809800000000003
(Val @ epoch 59) acc: 0.9965; ap: 0.9999963814767705
*************************
2024_08_10_20_32_07
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.9; ap: 99.6
(2 stylegan2 ) acc: 93.8; ap: 99.9
(3 biggan    ) acc: 75.8; ap: 87.6
(4 cyclegan  ) acc: 75.5; ap: 98.9
(5 stargan   ) acc: 98.4; ap: 100.0
(6 gaugan    ) acc: 76.8; ap: 82.9
(7 deepfake  ) acc: 75.6; ap: 82.3
(8 Mean      ) acc: 86.1; ap: 93.9
*************************
2024_08_10_20_36_05
2024_08_10_20_36_46 Train loss: 6.084576966713939e-07 at step: 270400 lr 0.00011809800000000003
2024_08_10_20_37_28 Train loss: 0.001403307425789535 at step: 270800 lr 0.00011809800000000003
2024_08_10_20_38_09 Train loss: 2.1340483726817183e-06 at step: 271200 lr 0.00011809800000000003
2024_08_10_20_38_51 Train loss: 1.0796466085594147e-06 at step: 271600 lr 0.00011809800000000003
2024_08_10_20_39_33 Train loss: 5.3844742656542974e-14 at step: 272000 lr 0.00011809800000000003
2024_08_10_20_40_14 Train loss: 8.984779015008826e-06 at step: 272400 lr 0.00011809800000000003
2024_08_10_20_40_56 Train loss: 2.994825010205204e-08 at step: 272800 lr 0.00011809800000000003
2024_08_10_20_41_37 Train loss: 2.3075388239199475e-14 at step: 273200 lr 0.00011809800000000003
2024_08_10_20_42_19 Train loss: 6.0235037381062284e-05 at step: 273600 lr 0.00011809800000000003
2024_08_10_20_43_02 Train loss: 2.4569968104515283e-07 at step: 274000 lr 0.00011809800000000003
2024_08_10_20_43_44 Train loss: 8.250487792960115e-11 at step: 274400 lr 0.00011809800000000003
saving the model at the end of epoch 60, iters 274561
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_latest.pth
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_60.pth
2024_08_10_20_44_01 changing lr at the end of epoch 60, iters 274561
*************************
Changing lr from 0.00011809800000000003 to 0.00010628820000000004
*************************
(Val @ epoch 60) acc: 0.997625; ap: 0.9999675210867165
*************************
2024_08_10_20_44_28
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.4; ap: 99.9
(2 stylegan2 ) acc: 95.8; ap: 99.9
(3 biggan    ) acc: 75.9; ap: 90.9
(4 cyclegan  ) acc: 61.7; ap: 98.5
(5 stargan   ) acc: 91.8; ap: 100.0
(6 gaugan    ) acc: 71.4; ap: 89.4
(7 deepfake  ) acc: 71.5; ap: 74.4
(8 Mean      ) acc: 82.8; ap: 94.1
*************************
2024_08_10_20_48_40
2024_08_10_20_49_10 Train loss: 4.470348358154297e-07 at step: 274800 lr 0.00010628820000000004
2024_08_10_20_49_52 Train loss: 1.217435527389732e-11 at step: 275200 lr 0.00010628820000000004
2024_08_10_20_50_33 Train loss: 2.3840411866871136e-09 at step: 275600 lr 0.00010628820000000004
2024_08_10_20_51_15 Train loss: 5.125999450683594e-06 at step: 276000 lr 0.00010628820000000004
2024_08_10_20_51_57 Train loss: 8.559988113297778e-13 at step: 276400 lr 0.00010628820000000004
2024_08_10_20_52_38 Train loss: 1.3674028055687959e-11 at step: 276800 lr 0.00010628820000000004
2024_08_10_20_53_20 Train loss: 2.984653110615909e-05 at step: 277200 lr 0.00010628820000000004
2024_08_10_20_54_02 Train loss: 4.2070449364885534e-14 at step: 277600 lr 0.00010628820000000004
2024_08_10_20_54_44 Train loss: 7.093252374357206e-11 at step: 278000 lr 0.00010628820000000004
2024_08_10_20_55_26 Train loss: 3.5948065146840236e-07 at step: 278400 lr 0.00010628820000000004
2024_08_10_20_56_07 Train loss: 2.0449684697346204e-13 at step: 278800 lr 0.00010628820000000004
(Val @ epoch 61) acc: 0.9975; ap: 0.9999430132295496
*************************
2024_08_10_20_57_02
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.6; ap: 99.9
(2 stylegan2 ) acc: 96.8; ap: 100.0
(3 biggan    ) acc: 79.6; ap: 90.0
(4 cyclegan  ) acc: 66.5; ap: 98.2
(5 stargan   ) acc: 95.9; ap: 100.0
(6 gaugan    ) acc: 79.0; ap: 85.3
(7 deepfake  ) acc: 64.8; ap: 66.3
(8 Mean      ) acc: 84.6; ap: 92.5
*************************
2024_08_10_21_01_06
2024_08_10_21_01_26 Train loss: 2.3134989532991312e-06 at step: 279200 lr 0.00010628820000000004
2024_08_10_21_02_07 Train loss: 2.980489810511244e-08 at step: 279600 lr 0.00010628820000000004
2024_08_10_21_02_49 Train loss: 4.016923753624724e-07 at step: 280000 lr 0.00010628820000000004
2024_08_10_21_03_31 Train loss: 5.312278937054382e-13 at step: 280400 lr 0.00010628820000000004
2024_08_10_21_04_12 Train loss: 1.6030510252562635e-11 at step: 280800 lr 0.00010628820000000004
2024_08_10_21_04_54 Train loss: 6.34867158844088e-13 at step: 281200 lr 0.00010628820000000004
2024_08_10_21_05_36 Train loss: 1.205698735295968e-10 at step: 281600 lr 0.00010628820000000004
2024_08_10_21_06_17 Train loss: 6.297116892107346e-17 at step: 282000 lr 0.00010628820000000004
2024_08_10_21_06_59 Train loss: 6.654709335668363e-13 at step: 282400 lr 0.00010628820000000004
2024_08_10_21_07_41 Train loss: 1.4376423228962265e-12 at step: 282800 lr 0.00010628820000000004
2024_08_10_21_08_23 Train loss: 2.1457672119140625e-06 at step: 283200 lr 0.00010628820000000004
(Val @ epoch 62) acc: 0.998125; ap: 0.9999925377121412
*************************
2024_08_10_21_09_27
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 93.9; ap: 99.0
(2 stylegan2 ) acc: 94.9; ap: 99.9
(3 biggan    ) acc: 80.3; ap: 88.9
(4 cyclegan  ) acc: 79.6; ap: 98.9
(5 stargan   ) acc: 99.2; ap: 100.0
(6 gaugan    ) acc: 78.8; ap: 83.1
(7 deepfake  ) acc: 68.5; ap: 75.0
(8 Mean      ) acc: 86.9; ap: 93.1
*************************
2024_08_10_21_13_36
2024_08_10_21_13_46 Train loss: 2.9206285034888424e-06 at step: 283600 lr 0.00010628820000000004
2024_08_10_21_14_28 Train loss: 3.2571334518394224e-09 at step: 284000 lr 0.00010628820000000004
2024_08_10_21_15_10 Train loss: 2.7165241085237712e-14 at step: 284400 lr 0.00010628820000000004
2024_08_10_21_15_52 Train loss: 1.1817064660135657e-06 at step: 284800 lr 0.00010628820000000004
2024_08_10_21_16_34 Train loss: 7.389135283459788e-14 at step: 285200 lr 0.00010628820000000004
2024_08_10_21_17_16 Train loss: 1.4417702058275306e-12 at step: 285600 lr 0.00010628820000000004
2024_08_10_21_17_58 Train loss: 1.19717055713231e-10 at step: 286000 lr 0.00010628820000000004
2024_08_10_21_18_44 Train loss: 2.38595532220387e-10 at step: 286400 lr 0.00010628820000000004
2024_08_10_21_19_33 Train loss: 1.2374503379180678e-15 at step: 286800 lr 0.00010628820000000004
2024_08_10_21_20_20 Train loss: 7.311012689345531e-13 at step: 287200 lr 0.00010628820000000004
2024_08_10_21_21_06 Train loss: 5.066480071036494e-07 at step: 287600 lr 0.00010628820000000004
2024_08_10_21_21_51 Train loss: 4.358042572649712e-12 at step: 288000 lr 0.00010628820000000004
(Val @ epoch 63) acc: 0.99575; ap: 0.9998292094692784
*************************
2024_08_10_21_22_39
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 91.2; ap: 99.6
(2 stylegan2 ) acc: 95.2; ap: 100.0
(3 biggan    ) acc: 80.3; ap: 90.8
(4 cyclegan  ) acc: 71.3; ap: 98.7
(5 stargan   ) acc: 97.4; ap: 100.0
(6 gaugan    ) acc: 79.2; ap: 85.8
(7 deepfake  ) acc: 62.6; ap: 68.1
(8 Mean      ) acc: 84.6; ap: 92.9
*************************
2024_08_10_21_26_57
2024_08_10_21_27_37 Train loss: 2.4174569013268865e-09 at step: 288400 lr 0.00010628820000000004
2024_08_10_21_28_18 Train loss: 6.875148415019794e-07 at step: 288800 lr 0.00010628820000000004
2024_08_10_21_29_00 Train loss: 1.995164931433823e-15 at step: 289200 lr 0.00010628820000000004
2024_08_10_21_29_42 Train loss: 3.267301963205682e-06 at step: 289600 lr 0.00010628820000000004
2024_08_10_21_30_23 Train loss: 3.2782554626464844e-07 at step: 290000 lr 0.00010628820000000004
2024_08_10_21_31_05 Train loss: 2.2773718466595483e-09 at step: 290400 lr 0.00010628820000000004
2024_08_10_21_31_47 Train loss: 4.356909798219899e-13 at step: 290800 lr 0.00010628820000000004
2024_08_10_21_32_28 Train loss: 7.64360742668696e-13 at step: 291200 lr 0.00010628820000000004
2024_08_10_21_33_10 Train loss: 3.385002500522205e-08 at step: 291600 lr 0.00010628820000000004
2024_08_10_21_33_52 Train loss: 2.8659217892330587e-15 at step: 292000 lr 0.00010628820000000004
2024_08_10_21_34_34 Train loss: 3.065995457518511e-08 at step: 292400 lr 0.00010628820000000004
(Val @ epoch 64) acc: 0.998375; ap: 0.99998966852825
*************************
2024_08_10_21_35_17
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 99.7
(2 stylegan2 ) acc: 97.0; ap: 100.0
(3 biggan    ) acc: 75.4; ap: 88.6
(4 cyclegan  ) acc: 69.2; ap: 98.3
(5 stargan   ) acc: 97.9; ap: 100.0
(6 gaugan    ) acc: 74.2; ap: 82.5
(7 deepfake  ) acc: 66.5; ap: 68.5
(8 Mean      ) acc: 84.1; ap: 92.2
*************************
2024_08_10_21_39_24
2024_08_10_21_39_54 Train loss: 1.5890307736299292e-08 at step: 292800 lr 0.00010628820000000004
2024_08_10_21_40_35 Train loss: 2.6769990668640276e-09 at step: 293200 lr 0.00010628820000000004
2024_08_10_21_41_17 Train loss: 1.4901162614933128e-07 at step: 293600 lr 0.00010628820000000004
2024_08_10_21_41_58 Train loss: 2.7855398117590588e-18 at step: 294000 lr 0.00010628820000000004
2024_08_10_21_42_40 Train loss: 1.3211320926131975e-10 at step: 294400 lr 0.00010628820000000004
2024_08_10_21_43_22 Train loss: 2.38437763755428e-07 at step: 294800 lr 0.00010628820000000004
2024_08_10_21_44_03 Train loss: 1.5528168750833515e-09 at step: 295200 lr 0.00010628820000000004
2024_08_10_21_44_45 Train loss: 8.942171803028032e-08 at step: 295600 lr 0.00010628820000000004
2024_08_10_21_45_26 Train loss: 4.881225734720829e-08 at step: 296000 lr 0.00010628820000000004
2024_08_10_21_46_08 Train loss: 2.254013198310645e-09 at step: 296400 lr 0.00010628820000000004
2024_08_10_21_46_50 Train loss: 6.089236764933695e-22 at step: 296800 lr 0.00010628820000000004
(Val @ epoch 65) acc: 0.998125; ap: 0.9999900106987101
*************************
2024_08_10_21_47_48
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.2; ap: 99.8
(2 stylegan2 ) acc: 96.8; ap: 100.0
(3 biggan    ) acc: 79.6; ap: 90.2
(4 cyclegan  ) acc: 68.9; ap: 98.2
(5 stargan   ) acc: 96.6; ap: 100.0
(6 gaugan    ) acc: 79.3; ap: 85.7
(7 deepfake  ) acc: 59.3; ap: 62.6
(8 Mean      ) acc: 84.1; ap: 92.1
*************************
2024_08_10_21_51_56
2024_08_10_21_52_16 Train loss: 6.355959476859714e-16 at step: 297200 lr 0.00010628820000000004
2024_08_10_21_52_57 Train loss: 1.5713314980778104e-13 at step: 297600 lr 0.00010628820000000004
2024_08_10_21_53_39 Train loss: 5.066394805908203e-07 at step: 298000 lr 0.00010628820000000004
2024_08_10_21_54_21 Train loss: 3.0549406320606876e-12 at step: 298400 lr 0.00010628820000000004
2024_08_10_21_55_03 Train loss: 3.4429783243167256e-12 at step: 298800 lr 0.00010628820000000004
2024_08_10_21_55_46 Train loss: 2.988354808053373e-08 at step: 299200 lr 0.00010628820000000004
2024_08_10_21_56_27 Train loss: 2.035807113165422e-11 at step: 299600 lr 0.00010628820000000004
2024_08_10_21_57_09 Train loss: 1.2018775308320073e-08 at step: 300000 lr 0.00010628820000000004
2024_08_10_21_57_52 Train loss: 2.980607973768201e-07 at step: 300400 lr 0.00010628820000000004
2024_08_10_21_58_36 Train loss: 2.576196905541145e-13 at step: 300800 lr 0.00010628820000000004
2024_08_10_21_59_20 Train loss: 6.327916413084722e-09 at step: 301200 lr 0.00010628820000000004
(Val @ epoch 66) acc: 0.999125; ap: 0.9999937060494981
*************************
2024_08_10_22_00_28
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 98.9
(2 stylegan2 ) acc: 98.8; ap: 100.0
(3 biggan    ) acc: 74.1; ap: 88.6
(4 cyclegan  ) acc: 64.3; ap: 97.9
(5 stargan   ) acc: 97.0; ap: 100.0
(6 gaugan    ) acc: 70.0; ap: 83.2
(7 deepfake  ) acc: 69.4; ap: 76.5
(8 Mean      ) acc: 83.2; ap: 93.1
*************************
2024_08_10_22_04_37
2024_08_10_22_04_46 Train loss: 5.3365089115686715e-05 at step: 301600 lr 0.00010628820000000004
2024_08_10_22_05_28 Train loss: 1.7543493413541e-06 at step: 302000 lr 0.00010628820000000004
2024_08_10_22_06_09 Train loss: 0.0065360404551029205 at step: 302400 lr 0.00010628820000000004
2024_08_10_22_06_51 Train loss: 1.90888689403923e-12 at step: 302800 lr 0.00010628820000000004
2024_08_10_22_07_33 Train loss: 2.4116515451311216e-09 at step: 303200 lr 0.00010628820000000004
2024_08_10_22_08_14 Train loss: 5.420670731837163e-06 at step: 303600 lr 0.00010628820000000004
2024_08_10_22_08_56 Train loss: 0.00010421872138977051 at step: 304000 lr 0.00010628820000000004
2024_08_10_22_09_37 Train loss: 4.470448686788586e-07 at step: 304400 lr 0.00010628820000000004
2024_08_10_22_10_19 Train loss: 1.6619214449065112e-08 at step: 304800 lr 0.00010628820000000004
2024_08_10_22_11_01 Train loss: 1.3880190399828685e-11 at step: 305200 lr 0.00010628820000000004
2024_08_10_22_11_43 Train loss: 1.9104604120911972e-08 at step: 305600 lr 0.00010628820000000004
2024_08_10_22_12_24 Train loss: 4.8637721192790195e-06 at step: 306000 lr 0.00010628820000000004
(Val @ epoch 67) acc: 0.997875; ap: 0.9999990020571705
*************************
2024_08_10_22_12_59
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 86.4; ap: 97.6
(2 stylegan2 ) acc: 85.6; ap: 99.8
(3 biggan    ) acc: 66.4; ap: 86.0
(4 cyclegan  ) acc: 60.9; ap: 98.8
(5 stargan   ) acc: 73.3; ap: 100.0
(6 gaugan    ) acc: 59.2; ap: 80.7
(7 deepfake  ) acc: 55.7; ap: 59.2
(8 Mean      ) acc: 73.4; ap: 90.3
*************************
2024_08_10_22_17_05
2024_08_10_22_17_45 Train loss: 2.607555700251396e-07 at step: 306400 lr 0.00010628820000000004
2024_08_10_22_18_27 Train loss: 5.961175730817558e-08 at step: 306800 lr 0.00010628820000000004
2024_08_10_22_19_09 Train loss: 6.561797931681213e-07 at step: 307200 lr 0.00010628820000000004
2024_08_10_22_19_50 Train loss: 6.765144007658819e-06 at step: 307600 lr 0.00010628820000000004
2024_08_10_22_20_32 Train loss: 8.941194096223626e-08 at step: 308000 lr 0.00010628820000000004
2024_08_10_22_21_13 Train loss: 1.3851281437382568e-06 at step: 308400 lr 0.00010628820000000004
2024_08_10_22_21_55 Train loss: 1.0065865918562622e-09 at step: 308800 lr 0.00010628820000000004
2024_08_10_22_22_37 Train loss: 9.328126907348633e-06 at step: 309200 lr 0.00010628820000000004
2024_08_10_22_23_18 Train loss: 1.490129619696745e-07 at step: 309600 lr 0.00010628820000000004
2024_08_10_22_24_03 Train loss: 6.538510705089706e-11 at step: 310000 lr 0.00010628820000000004
2024_08_10_22_24_47 Train loss: 4.608699913263544e-16 at step: 310400 lr 0.00010628820000000004
(Val @ epoch 68) acc: 0.994375; ap: 0.9997228312820094
*************************
2024_08_10_22_25_32
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 91.2; ap: 99.5
(2 stylegan2 ) acc: 92.6; ap: 99.9
(3 biggan    ) acc: 77.4; ap: 89.1
(4 cyclegan  ) acc: 72.3; ap: 98.7
(5 stargan   ) acc: 98.1; ap: 100.0
(6 gaugan    ) acc: 78.7; ap: 84.4
(7 deepfake  ) acc: 61.4; ap: 62.1
(8 Mean      ) acc: 83.9; ap: 91.7
*************************
2024_08_10_22_29_36
2024_08_10_22_30_05 Train loss: 1.9387529776038193e-11 at step: 310800 lr 0.00010628820000000004
2024_08_10_22_30_47 Train loss: 8.940719453676138e-08 at step: 311200 lr 0.00010628820000000004
2024_08_10_22_31_28 Train loss: 6.258495659494656e-07 at step: 311600 lr 0.00010628820000000004
2024_08_10_22_32_10 Train loss: 4.470358874186786e-07 at step: 312000 lr 0.00010628820000000004
2024_08_10_22_32_54 Train loss: 3.576281812911475e-07 at step: 312400 lr 0.00010628820000000004
2024_08_10_22_33_38 Train loss: 1.3225416296336334e-05 at step: 312800 lr 0.00010628820000000004
2024_08_10_22_34_22 Train loss: 3.055083652725443e-08 at step: 313200 lr 0.00010628820000000004
2024_08_10_22_35_06 Train loss: 4.870191219197295e-07 at step: 313600 lr 0.00010628820000000004
2024_08_10_22_35_49 Train loss: 2.9805256929194e-08 at step: 314000 lr 0.00010628820000000004
2024_08_10_22_36_31 Train loss: 4.374055007083655e-10 at step: 314400 lr 0.00010628820000000004
2024_08_10_22_37_12 Train loss: 1.3041915103428892e-08 at step: 314800 lr 0.00010628820000000004
(Val @ epoch 69) acc: 0.998375; ap: 0.9999941852697714
*************************
2024_08_10_22_38_06
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.9; ap: 99.5
(2 stylegan2 ) acc: 94.3; ap: 99.8
(3 biggan    ) acc: 72.5; ap: 89.5
(4 cyclegan  ) acc: 64.0; ap: 98.5
(5 stargan   ) acc: 92.8; ap: 99.9
(6 gaugan    ) acc: 69.1; ap: 90.0
(7 deepfake  ) acc: 62.3; ap: 60.7
(8 Mean      ) acc: 81.0; ap: 92.3
*************************
2024_08_10_22_42_11
2024_08_10_22_42_29 Train loss: 5.364440767152701e-07 at step: 315200 lr 0.00010628820000000004
2024_08_10_22_43_11 Train loss: 2.9817300628565135e-08 at step: 315600 lr 0.00010628820000000004
2024_08_10_22_43_52 Train loss: 1.5972918820938986e-10 at step: 316000 lr 0.00010628820000000004
2024_08_10_22_44_34 Train loss: 1.576959248506839e-15 at step: 316400 lr 0.00010628820000000004
2024_08_10_22_45_16 Train loss: 9.123645128283897e-08 at step: 316800 lr 0.00010628820000000004
2024_08_10_22_45_58 Train loss: 7.337409613228374e-08 at step: 317200 lr 0.00010628820000000004
2024_08_10_22_46_39 Train loss: 9.788368032781491e-12 at step: 317600 lr 0.00010628820000000004
2024_08_10_22_47_21 Train loss: 4.5513648405659524e-11 at step: 318000 lr 0.00010628820000000004
2024_08_10_22_48_03 Train loss: 5.136672076355788e-18 at step: 318400 lr 0.00010628820000000004
2024_08_10_22_48_45 Train loss: 3.3306312569036933e-11 at step: 318800 lr 0.00010628820000000004
2024_08_10_22_49_27 Train loss: 8.210242485873209e-10 at step: 319200 lr 0.00010628820000000004
2024_08_10_22_50_06 changing lr at the end of epoch 70, iters 319571
*************************
Changing lr from 0.00010628820000000004 to 9.565938000000004e-05
*************************
(Val @ epoch 70) acc: 0.99775; ap: 0.9999894180596531
*************************
2024_08_10_22_50_31
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.9; ap: 99.8
(2 stylegan2 ) acc: 95.8; ap: 100.0
(3 biggan    ) acc: 77.2; ap: 89.4
(4 cyclegan  ) acc: 66.4; ap: 98.4
(5 stargan   ) acc: 85.3; ap: 100.0
(6 gaugan    ) acc: 77.6; ap: 86.7
(7 deepfake  ) acc: 58.3; ap: 60.7
(8 Mean      ) acc: 81.5; ap: 91.9
*************************
2024_08_10_22_54_38
2024_08_10_22_54_46 Train loss: 2.7243623890171875e-07 at step: 319600 lr 9.565938000000004e-05
2024_08_10_22_55_28 Train loss: 6.09118266936548e-09 at step: 320000 lr 9.565938000000004e-05
2024_08_10_22_56_11 Train loss: 1.4137669923641738e-09 at step: 320400 lr 9.565938000000004e-05
2024_08_10_22_56_53 Train loss: 3.574543411917139e-14 at step: 320800 lr 9.565938000000004e-05
2024_08_10_22_57_38 Train loss: 0.00010108141577802598 at step: 321200 lr 9.565938000000004e-05
2024_08_10_22_58_20 Train loss: 2.4725372327338846e-07 at step: 321600 lr 9.565938000000004e-05
2024_08_10_22_59_01 Train loss: 1.7369248087276026e-15 at step: 322000 lr 9.565938000000004e-05
2024_08_10_22_59_43 Train loss: 2.7539186209235567e-12 at step: 322400 lr 9.565938000000004e-05
2024_08_10_23_00_25 Train loss: 2.5312837763358242e-12 at step: 322800 lr 9.565938000000004e-05
2024_08_10_23_01_06 Train loss: 9.64820535747854e-14 at step: 323200 lr 9.565938000000004e-05
2024_08_10_23_01_48 Train loss: 2.5788368065410403e-16 at step: 323600 lr 9.565938000000004e-05
2024_08_10_23_02_30 Train loss: 1.1797318330564188e-11 at step: 324000 lr 9.565938000000004e-05
(Val @ epoch 71) acc: 0.9965; ap: 0.9999565206189573
*************************
2024_08_10_23_03_02
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.9; ap: 99.4
(2 stylegan2 ) acc: 96.7; ap: 100.0
(3 biggan    ) acc: 73.0; ap: 88.8
(4 cyclegan  ) acc: 58.7; ap: 98.0
(5 stargan   ) acc: 82.0; ap: 100.0
(6 gaugan    ) acc: 66.5; ap: 84.1
(7 deepfake  ) acc: 62.5; ap: 63.1
(8 Mean      ) acc: 78.9; ap: 91.7
*************************
2024_08_10_23_07_09
2024_08_10_23_07_50 Train loss: 1.1828106272204764e-09 at step: 324400 lr 9.565938000000004e-05
2024_08_10_23_08_34 Train loss: 2.653654206097311e-12 at step: 324800 lr 9.565938000000004e-05
2024_08_10_23_09_18 Train loss: 7.748777761662495e-07 at step: 325200 lr 9.565938000000004e-05
2024_08_10_23_10_03 Train loss: 3.4535017261281414e-14 at step: 325600 lr 9.565938000000004e-05
2024_08_10_23_10_44 Train loss: 7.22501909239089e-15 at step: 326000 lr 9.565938000000004e-05
2024_08_10_23_11_26 Train loss: 3.1335298938950373e-11 at step: 326400 lr 9.565938000000004e-05
2024_08_10_23_12_08 Train loss: 1.2541448999106586e-12 at step: 326800 lr 9.565938000000004e-05
2024_08_10_23_12_49 Train loss: 1.3310968327573391e-08 at step: 327200 lr 9.565938000000004e-05
2024_08_10_23_13_31 Train loss: 3.3855637737723043e-10 at step: 327600 lr 9.565938000000004e-05
2024_08_10_23_14_12 Train loss: 5.721289553228814e-10 at step: 328000 lr 9.565938000000004e-05
2024_08_10_23_14_54 Train loss: 2.9802905032738636e-08 at step: 328400 lr 9.565938000000004e-05
(Val @ epoch 72) acc: 0.995875; ap: 0.999924956071973
*************************
2024_08_10_23_15_38
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.7; ap: 99.4
(2 stylegan2 ) acc: 96.3; ap: 100.0
(3 biggan    ) acc: 76.4; ap: 89.9
(4 cyclegan  ) acc: 63.7; ap: 98.5
(5 stargan   ) acc: 75.2; ap: 99.6
(6 gaugan    ) acc: 74.0; ap: 87.5
(7 deepfake  ) acc: 62.6; ap: 65.8
(8 Mean      ) acc: 80.0; ap: 92.6
*************************
2024_08_10_23_19_46
2024_08_10_23_20_14 Train loss: 6.853745408664721e-14 at step: 328800 lr 9.565938000000004e-05
2024_08_10_23_20_56 Train loss: 3.326092439664974e-12 at step: 329200 lr 9.565938000000004e-05
2024_08_10_23_21_38 Train loss: 2.448617524020147e-11 at step: 329600 lr 9.565938000000004e-05
2024_08_10_23_22_21 Train loss: 4.298125744206516e-11 at step: 330000 lr 9.565938000000004e-05
2024_08_10_23_23_04 Train loss: 1.7467822743810757e-08 at step: 330400 lr 9.565938000000004e-05
2024_08_10_23_23_46 Train loss: 8.940699558479537e-08 at step: 330800 lr 9.565938000000004e-05
2024_08_10_23_24_27 Train loss: 1.667454796461243e-08 at step: 331200 lr 9.565938000000004e-05
2024_08_10_23_25_09 Train loss: 1.673453334660735e-05 at step: 331600 lr 9.565938000000004e-05
2024_08_10_23_25_51 Train loss: 3.8522490752868066e-10 at step: 332000 lr 9.565938000000004e-05
2024_08_10_23_26_32 Train loss: 6.354534953789681e-11 at step: 332400 lr 9.565938000000004e-05
2024_08_10_23_27_14 Train loss: 1.1205422006943522e-10 at step: 332800 lr 9.565938000000004e-05
(Val @ epoch 73) acc: 0.9955; ap: 0.9997498836752192
*************************
2024_08_10_23_28_09
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 91.8; ap: 99.6
(2 stylegan2 ) acc: 96.8; ap: 100.0
(3 biggan    ) acc: 79.8; ap: 89.2
(4 cyclegan  ) acc: 71.7; ap: 98.7
(5 stargan   ) acc: 87.7; ap: 100.0
(6 gaugan    ) acc: 82.0; ap: 85.4
(7 deepfake  ) acc: 57.9; ap: 59.0
(8 Mean      ) acc: 83.4; ap: 91.5
*************************
2024_08_10_23_32_20
2024_08_10_23_32_38 Train loss: 2.539911336807421e-11 at step: 333200 lr 9.565938000000004e-05
2024_08_10_23_33_20 Train loss: 2.5053442258065814e-14 at step: 333600 lr 9.565938000000004e-05
2024_08_10_23_34_01 Train loss: 8.250605949651686e-16 at step: 334000 lr 9.565938000000004e-05
2024_08_10_23_34_43 Train loss: 1.2055306752856154e-10 at step: 334400 lr 9.565938000000004e-05
2024_08_10_23_35_25 Train loss: 4.2951739387397936e-10 at step: 334800 lr 9.565938000000004e-05
2024_08_10_23_36_06 Train loss: 5.960464477539063e-08 at step: 335200 lr 9.565938000000004e-05
2024_08_10_23_36_48 Train loss: 6.45968327717128e-08 at step: 335600 lr 9.565938000000004e-05
2024_08_10_23_37_29 Train loss: 1.0302854923371907e-11 at step: 336000 lr 9.565938000000004e-05
2024_08_10_23_38_11 Train loss: 2.760965284523409e-13 at step: 336400 lr 9.565938000000004e-05
2024_08_10_23_38_53 Train loss: 3.188099929264974e-16 at step: 336800 lr 9.565938000000004e-05
2024_08_10_23_39_34 Train loss: 4.6129731978705735e-12 at step: 337200 lr 9.565938000000004e-05
(Val @ epoch 74) acc: 0.997375; ap: 0.999950717554474
*************************
2024_08_10_23_40_42
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.1; ap: 99.7
(2 stylegan2 ) acc: 96.0; ap: 100.0
(3 biggan    ) acc: 78.6; ap: 89.8
(4 cyclegan  ) acc: 65.1; ap: 98.5
(5 stargan   ) acc: 82.3; ap: 100.0
(6 gaugan    ) acc: 76.3; ap: 84.2
(7 deepfake  ) acc: 59.7; ap: 67.1
(8 Mean      ) acc: 81.3; ap: 92.4
*************************
2024_08_10_23_44_53
2024_08_10_23_45_01 Train loss: 6.540701682523908e-14 at step: 337600 lr 9.565938000000004e-05
2024_08_10_23_45_43 Train loss: 9.435871470486745e-05 at step: 338000 lr 9.565938000000004e-05
2024_08_10_23_46_26 Train loss: 9.653219792526285e-14 at step: 338400 lr 9.565938000000004e-05
2024_08_10_23_47_08 Train loss: 4.459246340238199e-12 at step: 338800 lr 9.565938000000004e-05
2024_08_10_23_47_50 Train loss: 9.939605405406837e-08 at step: 339200 lr 9.565938000000004e-05
2024_08_10_23_48_33 Train loss: 2.908080101325563e-16 at step: 339600 lr 9.565938000000004e-05
2024_08_10_23_49_15 Train loss: 2.328876295368959e-13 at step: 340000 lr 9.565938000000004e-05
2024_08_10_23_49_57 Train loss: 2.813881567261412e-15 at step: 340400 lr 9.565938000000004e-05
2024_08_10_23_50_39 Train loss: 3.5638812999902805e-14 at step: 340800 lr 9.565938000000004e-05
2024_08_10_23_51_21 Train loss: 8.402087879800946e-21 at step: 341200 lr 9.565938000000004e-05
2024_08_10_23_52_02 Train loss: 4.2394296406733645e-10 at step: 341600 lr 9.565938000000004e-05
2024_08_10_23_52_44 Train loss: 9.056325325218495e-07 at step: 342000 lr 9.565938000000004e-05
(Val @ epoch 75) acc: 0.9975; ap: 0.999987097337031
*************************
2024_08_10_23_53_18
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.4; ap: 99.4
(2 stylegan2 ) acc: 97.5; ap: 100.0
(3 biggan    ) acc: 73.8; ap: 88.5
(4 cyclegan  ) acc: 64.5; ap: 98.3
(5 stargan   ) acc: 97.0; ap: 100.0
(6 gaugan    ) acc: 72.3; ap: 83.5
(7 deepfake  ) acc: 70.0; ap: 76.1
(8 Mean      ) acc: 83.4; ap: 93.2
*************************
2024_08_10_23_57_25
2024_08_10_23_58_04 Train loss: 6.897225013346997e-10 at step: 342400 lr 9.565938000000004e-05
2024_08_10_23_58_47 Train loss: 1.8139312209404512e-14 at step: 342800 lr 9.565938000000004e-05
2024_08_10_23_59_29 Train loss: 3.796852979335983e-16 at step: 343200 lr 9.565938000000004e-05
2024_08_11_00_00_11 Train loss: 6.395988805252273e-08 at step: 343600 lr 9.565938000000004e-05
2024_08_11_00_00_53 Train loss: 4.254567184669433e-12 at step: 344000 lr 9.565938000000004e-05
2024_08_11_00_01_34 Train loss: 2.2541996473519796e-20 at step: 344400 lr 9.565938000000004e-05
2024_08_11_00_02_16 Train loss: 4.791063474840485e-07 at step: 344800 lr 9.565938000000004e-05
2024_08_11_00_02_57 Train loss: 1.7160401455385e-07 at step: 345200 lr 9.565938000000004e-05
2024_08_11_00_03_39 Train loss: 2.1062890472565987e-14 at step: 345600 lr 9.565938000000004e-05
2024_08_11_00_04_21 Train loss: 0.00016160309314727783 at step: 346000 lr 9.565938000000004e-05
2024_08_11_00_05_02 Train loss: 2.980232949312267e-08 at step: 346400 lr 9.565938000000004e-05
(Val @ epoch 76) acc: 0.99775; ap: 0.9999680458497914
*************************
2024_08_11_00_05_47
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.9; ap: 99.5
(2 stylegan2 ) acc: 92.7; ap: 99.8
(3 biggan    ) acc: 78.0; ap: 90.3
(4 cyclegan  ) acc: 69.3; ap: 98.7
(5 stargan   ) acc: 98.7; ap: 100.0
(6 gaugan    ) acc: 78.5; ap: 86.5
(7 deepfake  ) acc: 74.6; ap: 80.7
(8 Mean      ) acc: 85.6; ap: 94.4
*************************
2024_08_11_00_09_52
2024_08_11_00_10_21 Train loss: 8.339071655427688e-10 at step: 346800 lr 9.565938000000004e-05
2024_08_11_00_11_03 Train loss: 8.899848739918337e-12 at step: 347200 lr 9.565938000000004e-05
2024_08_11_00_11_46 Train loss: 2.9802322387695312e-08 at step: 347600 lr 9.565938000000004e-05
2024_08_11_00_12_27 Train loss: 4.75578243452901e-09 at step: 348000 lr 9.565938000000004e-05
2024_08_11_00_13_09 Train loss: 9.058406829536361e-14 at step: 348400 lr 9.565938000000004e-05
2024_08_11_00_13_52 Train loss: 1.1551670731080321e-10 at step: 348800 lr 9.565938000000004e-05
2024_08_11_00_14_35 Train loss: 3.0781448353764396e-14 at step: 349200 lr 9.565938000000004e-05
2024_08_11_00_15_19 Train loss: 1.6613962928182423e-18 at step: 349600 lr 9.565938000000004e-05
2024_08_11_00_16_03 Train loss: 1.3720882918732968e-07 at step: 350000 lr 9.565938000000004e-05
2024_08_11_00_16_48 Train loss: 1.0446080977999372e-07 at step: 350400 lr 9.565938000000004e-05
2024_08_11_00_17_29 Train loss: 8.267749684745876e-15 at step: 350800 lr 9.565938000000004e-05
(Val @ epoch 77) acc: 0.997625; ap: 0.9999936419575224
*************************
2024_08_11_00_18_26
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 90.3; ap: 98.6
(2 stylegan2 ) acc: 96.1; ap: 100.0
(3 biggan    ) acc: 73.0; ap: 88.8
(4 cyclegan  ) acc: 63.7; ap: 98.7
(5 stargan   ) acc: 81.5; ap: 100.0
(6 gaugan    ) acc: 67.6; ap: 83.1
(7 deepfake  ) acc: 65.0; ap: 76.3
(8 Mean      ) acc: 79.6; ap: 93.2
*************************
2024_08_11_00_22_37
2024_08_11_00_22_56 Train loss: 4.870673566618848e-14 at step: 351200 lr 9.565938000000004e-05
2024_08_11_00_23_37 Train loss: 1.5904286556178704e-05 at step: 351600 lr 9.565938000000004e-05
2024_08_11_00_24_19 Train loss: 3.0799973682604787e-09 at step: 352000 lr 9.565938000000004e-05
2024_08_11_00_25_00 Train loss: 1.599834598131622e-09 at step: 352400 lr 9.565938000000004e-05
2024_08_11_00_25_42 Train loss: 3.0363227487839595e-09 at step: 352800 lr 9.565938000000004e-05
2024_08_11_00_26_24 Train loss: 5.319666929182176e-09 at step: 353200 lr 9.565938000000004e-05
2024_08_11_00_27_09 Train loss: 1.0328402133552572e-08 at step: 353600 lr 9.565938000000004e-05
2024_08_11_00_27_51 Train loss: 1.792380714960018e-07 at step: 354000 lr 9.565938000000004e-05
2024_08_11_00_28_35 Train loss: 6.854535286038299e-07 at step: 354400 lr 9.565938000000004e-05
2024_08_11_00_29_18 Train loss: 2.212149929903262e-08 at step: 354800 lr 9.565938000000004e-05
2024_08_11_00_30_00 Train loss: 9.5367431640625e-07 at step: 355200 lr 9.565938000000004e-05
(Val @ epoch 78) acc: 0.9985; ap: 0.9999862791381523
*************************
2024_08_11_00_31_06
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.5; ap: 99.7
(2 stylegan2 ) acc: 94.4; ap: 99.8
(3 biggan    ) acc: 81.3; ap: 92.0
(4 cyclegan  ) acc: 68.7; ap: 98.5
(5 stargan   ) acc: 98.4; ap: 100.0
(6 gaugan    ) acc: 79.7; ap: 88.5
(7 deepfake  ) acc: 73.0; ap: 76.4
(8 Mean      ) acc: 86.2; ap: 94.4
*************************
2024_08_11_00_35_12
2024_08_11_00_35_19 Train loss: 1.090802378977962e-10 at step: 355600 lr 9.565938000000004e-05
2024_08_11_00_36_00 Train loss: 2.9287570358358774e-18 at step: 356000 lr 9.565938000000004e-05
2024_08_11_00_36_42 Train loss: 2.377318679691598e-07 at step: 356400 lr 9.565938000000004e-05
2024_08_11_00_37_23 Train loss: 3.539631755936653e-22 at step: 356800 lr 9.565938000000004e-05
2024_08_11_00_38_05 Train loss: 5.002764046290218e-19 at step: 357200 lr 9.565938000000004e-05
2024_08_11_00_38_47 Train loss: 2.133528028065609e-16 at step: 357600 lr 9.565938000000004e-05
2024_08_11_00_39_28 Train loss: 5.96062577074008e-08 at step: 358000 lr 9.565938000000004e-05
2024_08_11_00_40_10 Train loss: 1.0224073324072158e-11 at step: 358400 lr 9.565938000000004e-05
2024_08_11_00_40_51 Train loss: 8.344650268554688e-07 at step: 358800 lr 9.565938000000004e-05
2024_08_11_00_41_33 Train loss: 8.14097875263542e-05 at step: 359200 lr 9.565938000000004e-05
2024_08_11_00_42_15 Train loss: 5.963689631016678e-08 at step: 359600 lr 9.565938000000004e-05
2024_08_11_00_42_56 Train loss: 2.479038641034492e-10 at step: 360000 lr 9.565938000000004e-05
(Val @ epoch 79) acc: 0.997875; ap: 0.9999757667021432
*************************
2024_08_11_00_43_31
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 92.7; ap: 99.5
(2 stylegan2 ) acc: 98.6; ap: 100.0
(3 biggan    ) acc: 77.8; ap: 89.6
(4 cyclegan  ) acc: 64.5; ap: 98.7
(5 stargan   ) acc: 84.7; ap: 100.0
(6 gaugan    ) acc: 73.7; ap: 84.1
(7 deepfake  ) acc: 59.0; ap: 62.0
(8 Mean      ) acc: 81.3; ap: 91.7
*************************
2024_08_11_00_47_39
2024_08_11_00_48_18 Train loss: 1.0082958908997144e-13 at step: 360400 lr 9.565938000000004e-05
2024_08_11_00_48_59 Train loss: 7.061631583583306e-19 at step: 360800 lr 9.565938000000004e-05
2024_08_11_00_49_41 Train loss: 4.551211024803742e-14 at step: 361200 lr 9.565938000000004e-05
2024_08_11_00_50_23 Train loss: 1.2805016395746209e-21 at step: 361600 lr 9.565938000000004e-05
2024_08_11_00_51_04 Train loss: 3.925553926364955e-08 at step: 362000 lr 9.565938000000004e-05
2024_08_11_00_51_46 Train loss: 5.011230722105995e-15 at step: 362400 lr 9.565938000000004e-05
2024_08_11_00_52_28 Train loss: 6.565633453581299e-13 at step: 362800 lr 9.565938000000004e-05
2024_08_11_00_53_10 Train loss: 9.377607579352798e-10 at step: 363200 lr 9.565938000000004e-05
2024_08_11_00_53_51 Train loss: 7.831949539317644e-15 at step: 363600 lr 9.565938000000004e-05
2024_08_11_00_54_33 Train loss: 1.4191960268884957e-16 at step: 364000 lr 9.565938000000004e-05
2024_08_11_00_55_15 Train loss: 1.457545327108404e-14 at step: 364400 lr 9.565938000000004e-05
saving the model at the end of epoch 80, iters 364581
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_latest.pth
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_80.pth
2024_08_11_00_55_34 changing lr at the end of epoch 80, iters 364581
*************************
Changing lr from 9.565938000000004e-05 to 8.609344200000004e-05
*************************
(Val @ epoch 80) acc: 0.99775; ap: 0.9999881393894112
*************************
2024_08_11_00_56_02
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 90.1; ap: 99.5
(2 stylegan2 ) acc: 94.7; ap: 100.0
(3 biggan    ) acc: 77.5; ap: 88.4
(4 cyclegan  ) acc: 67.8; ap: 98.7
(5 stargan   ) acc: 90.3; ap: 100.0
(6 gaugan    ) acc: 77.0; ap: 84.5
(7 deepfake  ) acc: 53.5; ap: 58.2
(8 Mean      ) acc: 81.4; ap: 91.2
*************************
2024_08_11_01_00_08
2024_08_11_01_00_36 Train loss: 1.2933437268288446e-18 at step: 364800 lr 8.609344200000004e-05
2024_08_11_01_01_18 Train loss: 1.050447486020947e-15 at step: 365200 lr 8.609344200000004e-05
2024_08_11_01_01_59 Train loss: 3.215867502848191e-11 at step: 365600 lr 8.609344200000004e-05
2024_08_11_01_02_41 Train loss: 6.390656910994566e-14 at step: 366000 lr 8.609344200000004e-05
2024_08_11_01_03_23 Train loss: 3.617648403064777e-09 at step: 366400 lr 8.609344200000004e-05
2024_08_11_01_04_04 Train loss: 3.7346258626846235e-14 at step: 366800 lr 8.609344200000004e-05
2024_08_11_01_04_46 Train loss: 8.061044887153201e-12 at step: 367200 lr 8.609344200000004e-05
2024_08_11_01_05_28 Train loss: 0.016549143940210342 at step: 367600 lr 8.609344200000004e-05
2024_08_11_01_06_09 Train loss: 5.03277286512116e-09 at step: 368000 lr 8.609344200000004e-05
2024_08_11_01_06_51 Train loss: 4.95975740477661e-08 at step: 368400 lr 8.609344200000004e-05
2024_08_11_01_07_32 Train loss: 6.793689692737384e-16 at step: 368800 lr 8.609344200000004e-05
(Val @ epoch 81) acc: 0.998125; ap: 0.9999898242946897
*************************
2024_08_11_01_08_30
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 91.7; ap: 99.1
(2 stylegan2 ) acc: 94.9; ap: 99.8
(3 biggan    ) acc: 70.4; ap: 90.2
(4 cyclegan  ) acc: 58.5; ap: 98.4
(5 stargan   ) acc: 78.7; ap: 100.0
(6 gaugan    ) acc: 65.0; ap: 89.5
(7 deepfake  ) acc: 61.6; ap: 63.9
(8 Mean      ) acc: 77.6; ap: 92.6
*************************
2024_08_11_01_12_42
2024_08_11_01_13_00 Train loss: 3.24692991853226e-05 at step: 369200 lr 8.609344200000004e-05
2024_08_11_01_13_41 Train loss: 1.4904449585628754e-07 at step: 369600 lr 8.609344200000004e-05
2024_08_11_01_14_23 Train loss: 3.707949858267057e-08 at step: 370000 lr 8.609344200000004e-05
2024_08_11_01_15_05 Train loss: 3.4516792202232693e-10 at step: 370400 lr 8.609344200000004e-05
2024_08_11_01_15_46 Train loss: 1.3843178749084473e-05 at step: 370800 lr 8.609344200000004e-05
2024_08_11_01_16_28 Train loss: 1.6278754602985779e-12 at step: 371200 lr 8.609344200000004e-05
2024_08_11_01_17_10 Train loss: 8.334691986057463e-13 at step: 371600 lr 8.609344200000004e-05
2024_08_11_01_17_52 Train loss: 2.384185791015625e-06 at step: 372000 lr 8.609344200000004e-05
2024_08_11_01_18_33 Train loss: 1.2060316634254775e-10 at step: 372400 lr 8.609344200000004e-05
2024_08_11_01_19_15 Train loss: 0.0005984694580547512 at step: 372800 lr 8.609344200000004e-05
2024_08_11_01_19_58 Train loss: 2.348090473330188e-17 at step: 373200 lr 8.609344200000004e-05
(Val @ epoch 82) acc: 0.9985; ap: 0.9999977477420706
*************************
2024_08_11_01_21_05
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 90.6; ap: 99.2
(2 stylegan2 ) acc: 94.5; ap: 99.9
(3 biggan    ) acc: 72.2; ap: 87.1
(4 cyclegan  ) acc: 61.5; ap: 98.3
(5 stargan   ) acc: 67.7; ap: 98.3
(6 gaugan    ) acc: 71.0; ap: 87.2
(7 deepfake  ) acc: 60.2; ap: 61.9
(8 Mean      ) acc: 77.2; ap: 91.5
*************************
2024_08_11_01_25_21
2024_08_11_01_25_27 Train loss: 5.700653744161793e-18 at step: 373600 lr 8.609344200000004e-05
2024_08_11_01_26_12 Train loss: 2.1872750277651676e-09 at step: 374000 lr 8.609344200000004e-05
2024_08_11_01_26_56 Train loss: 5.2502748547659465e-15 at step: 374400 lr 8.609344200000004e-05
2024_08_11_01_27_38 Train loss: 1.5966131953863705e-13 at step: 374800 lr 8.609344200000004e-05
2024_08_11_01_28_23 Train loss: 1.5776208879449436e-18 at step: 375200 lr 8.609344200000004e-05
2024_08_11_01_29_07 Train loss: 4.925013101877464e-14 at step: 375600 lr 8.609344200000004e-05
2024_08_11_01_29_49 Train loss: 1.1920928955078125e-07 at step: 376000 lr 8.609344200000004e-05
2024_08_11_01_30_30 Train loss: 2.7472566322545022e-12 at step: 376400 lr 8.609344200000004e-05
2024_08_11_01_31_12 Train loss: 2.2128615384531258e-08 at step: 376800 lr 8.609344200000004e-05
2024_08_11_01_31_54 Train loss: 3.6017768086614443e-13 at step: 377200 lr 8.609344200000004e-05
2024_08_11_01_32_41 Train loss: 2.3084159693098627e-05 at step: 377600 lr 8.609344200000004e-05
2024_08_11_01_33_26 Train loss: 2.641548004639338e-12 at step: 378000 lr 8.609344200000004e-05
(Val @ epoch 83) acc: 0.998625; ap: 0.9999927782944918
*************************
2024_08_11_01_34_05
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 90.6; ap: 99.0
(2 stylegan2 ) acc: 93.6; ap: 99.9
(3 biggan    ) acc: 69.8; ap: 87.6
(4 cyclegan  ) acc: 59.8; ap: 98.6
(5 stargan   ) acc: 66.5; ap: 99.6
(6 gaugan    ) acc: 67.1; ap: 88.2
(7 deepfake  ) acc: 63.0; ap: 66.0
(8 Mean      ) acc: 76.3; ap: 92.4
*************************
2024_08_11_01_38_18
2024_08_11_01_38_56 Train loss: 0.03087013214826584 at step: 378400 lr 8.609344200000004e-05
2024_08_11_01_39_38 Train loss: 3.904199729731772e-06 at step: 378800 lr 8.609344200000004e-05
2024_08_11_01_40_20 Train loss: 2.2408628410630627e-06 at step: 379200 lr 8.609344200000004e-05
2024_08_11_01_41_03 Train loss: 2.2112489617143183e-10 at step: 379600 lr 8.609344200000004e-05
2024_08_11_01_41_46 Train loss: 5.873340432304948e-15 at step: 380000 lr 8.609344200000004e-05
2024_08_11_01_42_29 Train loss: 2.980235436211842e-08 at step: 380400 lr 8.609344200000004e-05
2024_08_11_01_43_12 Train loss: 4.8545240743935736e-11 at step: 380800 lr 8.609344200000004e-05
2024_08_11_01_43_53 Train loss: 9.736860780880916e-16 at step: 381200 lr 8.609344200000004e-05
2024_08_11_01_44_35 Train loss: 1.076311839653954e-08 at step: 381600 lr 8.609344200000004e-05
2024_08_11_01_45_17 Train loss: 3.7471592804649845e-05 at step: 382000 lr 8.609344200000004e-05
2024_08_11_01_45_59 Train loss: 3.964722786322728e-15 at step: 382400 lr 8.609344200000004e-05
(Val @ epoch 84) acc: 0.996875; ap: 0.9999642289060645
*************************
2024_08_11_01_46_45
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.5; ap: 99.9
(2 stylegan2 ) acc: 96.1; ap: 100.0
(3 biggan    ) acc: 80.5; ap: 90.3
(4 cyclegan  ) acc: 73.6; ap: 98.5
(5 stargan   ) acc: 98.1; ap: 100.0
(6 gaugan    ) acc: 84.8; ap: 88.6
(7 deepfake  ) acc: 54.3; ap: 59.5
(8 Mean      ) acc: 85.2; ap: 92.1
*************************
2024_08_11_01_50_57
2024_08_11_01_51_26 Train loss: 1.848203656294018e-15 at step: 382800 lr 8.609344200000004e-05
2024_08_11_01_52_08 Train loss: 7.159993931082909e-09 at step: 383200 lr 8.609344200000004e-05
2024_08_11_01_52_49 Train loss: 0.0004444562364369631 at step: 383600 lr 8.609344200000004e-05
2024_08_11_01_53_31 Train loss: 5.855855574543586e-19 at step: 384000 lr 8.609344200000004e-05
2024_08_11_01_54_13 Train loss: 2.4217248206781505e-09 at step: 384400 lr 8.609344200000004e-05
2024_08_11_01_54_54 Train loss: 2.384185791015625e-07 at step: 384800 lr 8.609344200000004e-05
2024_08_11_01_55_36 Train loss: 6.298753185696171e-12 at step: 385200 lr 8.609344200000004e-05
2024_08_11_01_56_18 Train loss: 5.4555130554680316e-15 at step: 385600 lr 8.609344200000004e-05
2024_08_11_01_56_59 Train loss: 6.84844958520614e-10 at step: 386000 lr 8.609344200000004e-05
2024_08_11_01_57_42 Train loss: 3.122963381494738e-14 at step: 386400 lr 8.609344200000004e-05
2024_08_11_01_58_25 Train loss: 3.302032425267498e-08 at step: 386800 lr 8.609344200000004e-05
(Val @ epoch 85) acc: 0.9985; ap: 0.9999899287118635
*************************
2024_08_11_01_59_23
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.5; ap: 99.7
(2 stylegan2 ) acc: 97.2; ap: 100.0
(3 biggan    ) acc: 80.5; ap: 91.2
(4 cyclegan  ) acc: 68.5; ap: 98.6
(5 stargan   ) acc: 90.6; ap: 99.9
(6 gaugan    ) acc: 82.4; ap: 89.0
(7 deepfake  ) acc: 64.3; ap: 66.7
(8 Mean      ) acc: 84.7; ap: 93.1
*************************
2024_08_11_02_03_29
2024_08_11_02_03_47 Train loss: 2.9228954326332435e-12 at step: 387200 lr 8.609344200000004e-05
2024_08_11_02_04_29 Train loss: 3.46061374001394e-11 at step: 387600 lr 8.609344200000004e-05
2024_08_11_02_05_11 Train loss: 1.471633405414341e-11 at step: 388000 lr 8.609344200000004e-05
2024_08_11_02_05_52 Train loss: 8.24807277056798e-14 at step: 388400 lr 8.609344200000004e-05
2024_08_11_02_06_34 Train loss: 7.8740889364326e-09 at step: 388800 lr 8.609344200000004e-05
2024_08_11_02_07_15 Train loss: 1.1904715435662183e-08 at step: 389200 lr 8.609344200000004e-05
2024_08_11_02_07_57 Train loss: 1.2827063087909873e-13 at step: 389600 lr 8.609344200000004e-05
2024_08_11_02_08_38 Train loss: 8.025193842797762e-10 at step: 390000 lr 8.609344200000004e-05
2024_08_11_02_09_20 Train loss: 2.0623918850494778e-17 at step: 390400 lr 8.609344200000004e-05
2024_08_11_02_10_02 Train loss: 2.921132455526278e-15 at step: 390800 lr 8.609344200000004e-05
2024_08_11_02_10_43 Train loss: 1.1711156275850954e-06 at step: 391200 lr 8.609344200000004e-05
(Val @ epoch 86) acc: 0.99675; ap: 0.9999725737074815
*************************
2024_08_11_02_11_55
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 89.4; ap: 99.0
(2 stylegan2 ) acc: 94.2; ap: 100.0
(3 biggan    ) acc: 67.3; ap: 86.8
(4 cyclegan  ) acc: 56.9; ap: 98.4
(5 stargan   ) acc: 62.9; ap: 99.6
(6 gaugan    ) acc: 59.3; ap: 83.6
(7 deepfake  ) acc: 56.9; ap: 62.8
(8 Mean      ) acc: 73.3; ap: 91.3
*************************
2024_08_11_02_16_04
2024_08_11_02_16_11 Train loss: 8.51768845677725e-07 at step: 391600 lr 8.609344200000004e-05
2024_08_11_02_16_52 Train loss: 7.887085604029955e-23 at step: 392000 lr 8.609344200000004e-05
2024_08_11_02_17_34 Train loss: 3.986060619354248e-05 at step: 392400 lr 8.609344200000004e-05
2024_08_11_02_18_16 Train loss: 1.8002752066870187e-11 at step: 392800 lr 8.609344200000004e-05
2024_08_11_02_18_58 Train loss: 6.91771306904343e-10 at step: 393200 lr 8.609344200000004e-05
2024_08_11_02_19_40 Train loss: 6.72240022352291e-17 at step: 393600 lr 8.609344200000004e-05
2024_08_11_02_20_21 Train loss: 1.8974291582374292e-10 at step: 394000 lr 8.609344200000004e-05
2024_08_11_02_21_03 Train loss: 3.963917003880005e-14 at step: 394400 lr 8.609344200000004e-05
2024_08_11_02_21_46 Train loss: 6.369527821847853e-11 at step: 394800 lr 8.609344200000004e-05
2024_08_11_02_22_31 Train loss: 1.949328198858069e-10 at step: 395200 lr 8.609344200000004e-05
2024_08_11_02_23_17 Train loss: 4.3083668143512055e-13 at step: 395600 lr 8.609344200000004e-05
2024_08_11_02_24_03 Train loss: 3.8311164546691546e-17 at step: 396000 lr 8.609344200000004e-05
(Val @ epoch 87) acc: 0.99775; ap: 0.9999857916368192
*************************
2024_08_11_02_24_38
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 90.7; ap: 99.4
(2 stylegan2 ) acc: 92.0; ap: 99.9
(3 biggan    ) acc: 83.9; ap: 90.9
(4 cyclegan  ) acc: 79.8; ap: 98.6
(5 stargan   ) acc: 97.6; ap: 100.0
(6 gaugan    ) acc: 87.4; ap: 89.1
(7 deepfake  ) acc: 52.0; ap: 58.6
(8 Mean      ) acc: 85.4; ap: 92.1
*************************
2024_08_11_02_28_43
2024_08_11_02_29_20 Train loss: 1.6046981521355974e-09 at step: 396400 lr 8.609344200000004e-05
2024_08_11_02_30_01 Train loss: 4.082918167114258e-06 at step: 396800 lr 8.609344200000004e-05
2024_08_11_02_30_43 Train loss: 1.0227797275774719e-07 at step: 397200 lr 8.609344200000004e-05
2024_08_11_02_31_24 Train loss: 4.072741764064164e-14 at step: 397600 lr 8.609344200000004e-05
2024_08_11_02_32_09 Train loss: 5.960464477539063e-08 at step: 398000 lr 8.609344200000004e-05
2024_08_11_02_32_50 Train loss: 3.1177878746617427e-13 at step: 398400 lr 8.609344200000004e-05
2024_08_11_02_33_32 Train loss: 5.068069445016143e-13 at step: 398800 lr 8.609344200000004e-05
2024_08_11_02_34_16 Train loss: 1.422360594918997e-15 at step: 399200 lr 8.609344200000004e-05
2024_08_11_02_34_57 Train loss: 2.5748434485353862e-14 at step: 399600 lr 8.609344200000004e-05
2024_08_11_02_35_39 Train loss: 6.474606184525555e-09 at step: 400000 lr 8.609344200000004e-05
2024_08_11_02_36_21 Train loss: 1.3984370236537068e-18 at step: 400400 lr 8.609344200000004e-05
(Val @ epoch 88) acc: 0.997375; ap: 0.9999934524349864
*************************
2024_08_11_02_37_06
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 90.0; ap: 99.5
(2 stylegan2 ) acc: 94.4; ap: 100.0
(3 biggan    ) acc: 73.1; ap: 87.8
(4 cyclegan  ) acc: 64.4; ap: 98.3
(5 stargan   ) acc: 80.9; ap: 100.0
(6 gaugan    ) acc: 71.8; ap: 83.6
(7 deepfake  ) acc: 52.8; ap: 61.9
(8 Mean      ) acc: 78.4; ap: 91.4
*************************
2024_08_11_02_41_11
2024_08_11_02_41_38 Train loss: 4.470348358154297e-07 at step: 400800 lr 8.609344200000004e-05
2024_08_11_02_42_20 Train loss: 7.246168942653952e-11 at step: 401200 lr 8.609344200000004e-05
2024_08_11_02_43_01 Train loss: 1.8851673075914732e-06 at step: 401600 lr 8.609344200000004e-05
2024_08_11_02_43_43 Train loss: 1.7881393432617188e-07 at step: 402000 lr 8.609344200000004e-05
2024_08_11_02_44_25 Train loss: 5.566524463596068e-10 at step: 402400 lr 8.609344200000004e-05
2024_08_11_02_45_06 Train loss: 1.2458842846285734e-09 at step: 402800 lr 8.609344200000004e-05
2024_08_11_02_45_48 Train loss: 0.03644262254238129 at step: 403200 lr 8.609344200000004e-05
2024_08_11_02_46_29 Train loss: 3.199248643781516e-09 at step: 403600 lr 8.609344200000004e-05
2024_08_11_02_47_11 Train loss: 8.940732243445382e-08 at step: 404000 lr 8.609344200000004e-05
2024_08_11_02_47_53 Train loss: 1.913586095136921e-10 at step: 404400 lr 8.609344200000004e-05
2024_08_11_02_48_34 Train loss: 1.6976352412712004e-07 at step: 404800 lr 8.609344200000004e-05
(Val @ epoch 89) acc: 0.996375; ap: 0.9999793542821778
*************************
2024_08_11_02_49_31
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 90.7; ap: 99.5
(2 stylegan2 ) acc: 94.1; ap: 99.9
(3 biggan    ) acc: 72.2; ap: 89.4
(4 cyclegan  ) acc: 62.0; ap: 98.9
(5 stargan   ) acc: 65.3; ap: 99.7
(6 gaugan    ) acc: 73.1; ap: 88.9
(7 deepfake  ) acc: 57.4; ap: 62.2
(8 Mean      ) acc: 76.8; ap: 92.3
*************************
2024_08_11_02_53_35
2024_08_11_02_53_51 Train loss: 7.735008189513337e-10 at step: 405200 lr 8.609344200000004e-05
2024_08_11_02_54_32 Train loss: 5.391932816918323e-17 at step: 405600 lr 8.609344200000004e-05
2024_08_11_02_55_14 Train loss: 3.721617632124197e-12 at step: 406000 lr 8.609344200000004e-05
2024_08_11_02_55_55 Train loss: 1.9711706045200117e-08 at step: 406400 lr 8.609344200000004e-05
2024_08_11_02_56_37 Train loss: 1.4465573183741753e-09 at step: 406800 lr 8.609344200000004e-05
2024_08_11_02_57_19 Train loss: 3.7198002456051427e-09 at step: 407200 lr 8.609344200000004e-05
2024_08_11_02_58_00 Train loss: 2.1457672119140625e-06 at step: 407600 lr 8.609344200000004e-05
2024_08_11_02_58_42 Train loss: 1.1294150293814087e-13 at step: 408000 lr 8.609344200000004e-05
2024_08_11_02_59_24 Train loss: 9.591036298495226e-12 at step: 408400 lr 8.609344200000004e-05
2024_08_11_03_00_05 Train loss: 1.9210938395630706e-10 at step: 408800 lr 8.609344200000004e-05
2024_08_11_03_00_47 Train loss: 8.072270309447785e-18 at step: 409200 lr 8.609344200000004e-05
2024_08_11_03_01_28 changing lr at the end of epoch 90, iters 409591
*************************
Changing lr from 8.609344200000004e-05 to 7.748409780000004e-05
*************************
(Val @ epoch 90) acc: 0.997125; ap: 0.9998756017651035
*************************
2024_08_11_03_01_54
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.7; ap: 99.8
(2 stylegan2 ) acc: 98.2; ap: 100.0
(3 biggan    ) acc: 82.3; ap: 90.5
(4 cyclegan  ) acc: 75.4; ap: 98.6
(5 stargan   ) acc: 96.6; ap: 100.0
(6 gaugan    ) acc: 82.3; ap: 84.5
(7 deepfake  ) acc: 68.2; ap: 74.0
(8 Mean      ) acc: 87.2; ap: 93.4
*************************
2024_08_11_03_06_05
2024_08_11_03_06_10 Train loss: 2.5657072495223476e-13 at step: 409600 lr 7.748409780000004e-05
2024_08_11_03_06_52 Train loss: 5.960464477539063e-08 at step: 410000 lr 7.748409780000004e-05
2024_08_11_03_07_34 Train loss: 1.660966086891591e-10 at step: 410400 lr 7.748409780000004e-05
2024_08_11_03_08_15 Train loss: 4.688528321139529e-08 at step: 410800 lr 7.748409780000004e-05
2024_08_11_03_08_57 Train loss: 4.915087610796092e-22 at step: 411200 lr 7.748409780000004e-05
2024_08_11_03_09_38 Train loss: 1.4843844469564216e-13 at step: 411600 lr 7.748409780000004e-05
2024_08_11_03_10_20 Train loss: 3.491155118901662e-13 at step: 412000 lr 7.748409780000004e-05
2024_08_11_03_11_02 Train loss: 1.0570702299594892e-11 at step: 412400 lr 7.748409780000004e-05
2024_08_11_03_11_43 Train loss: 1.909592953599007e-13 at step: 412800 lr 7.748409780000004e-05
2024_08_11_03_12_25 Train loss: 2.8415012010007956e-10 at step: 413200 lr 7.748409780000004e-05
2024_08_11_03_13_06 Train loss: 9.585206760254206e-12 at step: 413600 lr 7.748409780000004e-05
2024_08_11_03_13_48 Train loss: 2.8163673972033667e-15 at step: 414000 lr 7.748409780000004e-05
(Val @ epoch 91) acc: 0.9965; ap: 0.9999249305315521
*************************
2024_08_11_03_14_25
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 93.6; ap: 99.0
(2 stylegan2 ) acc: 93.4; ap: 99.8
(3 biggan    ) acc: 72.4; ap: 88.5
(4 cyclegan  ) acc: 63.6; ap: 98.2
(5 stargan   ) acc: 84.9; ap: 99.8
(6 gaugan    ) acc: 65.1; ap: 82.9
(7 deepfake  ) acc: 70.5; ap: 89.4
(8 Mean      ) acc: 80.4; ap: 94.7
*************************
2024_08_11_03_18_38
2024_08_11_03_19_15 Train loss: 4.72407638859142e-16 at step: 414400 lr 7.748409780000004e-05
2024_08_11_03_19_56 Train loss: 8.034097340311672e-15 at step: 414800 lr 7.748409780000004e-05
2024_08_11_03_20_38 Train loss: 1.921924669108857e-13 at step: 415200 lr 7.748409780000004e-05
2024_08_11_03_21_19 Train loss: 1.0576873307310136e-14 at step: 415600 lr 7.748409780000004e-05
2024_08_11_03_22_01 Train loss: 3.3081294077419443e-06 at step: 416000 lr 7.748409780000004e-05
2024_08_11_03_22_43 Train loss: 1.6966522566734825e-16 at step: 416400 lr 7.748409780000004e-05
2024_08_11_03_23_24 Train loss: 2.9802428969105677e-08 at step: 416800 lr 7.748409780000004e-05
2024_08_11_03_24_06 Train loss: 1.7180552536188998e-09 at step: 417200 lr 7.748409780000004e-05
2024_08_11_03_24_48 Train loss: 6.499338667254051e-08 at step: 417600 lr 7.748409780000004e-05
2024_08_11_03_25_31 Train loss: 5.724705865600699e-12 at step: 418000 lr 7.748409780000004e-05
2024_08_11_03_26_17 Train loss: 1.7343648295309322e-08 at step: 418400 lr 7.748409780000004e-05
(Val @ epoch 92) acc: 0.99675; ap: 0.999918176520242
*************************
2024_08_11_03_27_08
(0 progan    ) acc: 99.7; ap: 100.0
(1 stylegan  ) acc: 93.7; ap: 99.7
(2 stylegan2 ) acc: 96.6; ap: 99.9
(3 biggan    ) acc: 73.9; ap: 87.0
(4 cyclegan  ) acc: 64.0; ap: 97.8
(5 stargan   ) acc: 78.5; ap: 99.9
(6 gaugan    ) acc: 72.7; ap: 82.9
(7 deepfake  ) acc: 62.9; ap: 68.3
(8 Mean      ) acc: 80.3; ap: 91.9
*************************
2024_08_11_03_31_14
2024_08_11_03_31_40 Train loss: 8.941032092479873e-08 at step: 418800 lr 7.748409780000004e-05
2024_08_11_03_32_22 Train loss: 1.1920928955078125e-07 at step: 419200 lr 7.748409780000004e-05
2024_08_11_03_33_04 Train loss: 3.2414613528042313e-12 at step: 419600 lr 7.748409780000004e-05
2024_08_11_03_33_45 Train loss: 4.903700710015229e-17 at step: 420000 lr 7.748409780000004e-05
2024_08_11_03_34_27 Train loss: 2.6822101517609553e-07 at step: 420400 lr 7.748409780000004e-05
2024_08_11_03_35_09 Train loss: 9.922855451812568e-16 at step: 420800 lr 7.748409780000004e-05
2024_08_11_03_35_50 Train loss: 3.6845272519863206e-14 at step: 421200 lr 7.748409780000004e-05
2024_08_11_03_36_32 Train loss: 1.304239962557352e-11 at step: 421600 lr 7.748409780000004e-05
2024_08_11_03_37_14 Train loss: 6.004262079797309e-15 at step: 422000 lr 7.748409780000004e-05
2024_08_11_03_37_55 Train loss: 3.765187273074844e-09 at step: 422400 lr 7.748409780000004e-05
2024_08_11_03_38_37 Train loss: 2.0184769411457637e-08 at step: 422800 lr 7.748409780000004e-05
(Val @ epoch 93) acc: 0.996375; ap: 0.9999736518405986
*************************
2024_08_11_03_39_34
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 93.2; ap: 99.8
(2 stylegan2 ) acc: 93.0; ap: 99.9
(3 biggan    ) acc: 75.6; ap: 90.6
(4 cyclegan  ) acc: 65.7; ap: 98.7
(5 stargan   ) acc: 81.0; ap: 100.0
(6 gaugan    ) acc: 73.5; ap: 88.2
(7 deepfake  ) acc: 75.9; ap: 82.5
(8 Mean      ) acc: 82.2; ap: 95.0
*************************
2024_08_11_03_43_40
2024_08_11_03_43_57 Train loss: 8.467347069535721e-15 at step: 423200 lr 7.748409780000004e-05
2024_08_11_03_44_39 Train loss: 7.699805203154053e-12 at step: 423600 lr 7.748409780000004e-05
2024_08_11_03_45_20 Train loss: 1.5563845323640635e-08 at step: 424000 lr 7.748409780000004e-05
2024_08_11_03_46_02 Train loss: 4.5743944833297035e-17 at step: 424400 lr 7.748409780000004e-05
2024_08_11_03_46_43 Train loss: 1.1633699645990925e-12 at step: 424800 lr 7.748409780000004e-05
2024_08_11_03_47_25 Train loss: 7.476053399935699e-15 at step: 425200 lr 7.748409780000004e-05
2024_08_11_03_48_07 Train loss: 3.0958523938862115e-11 at step: 425600 lr 7.748409780000004e-05
2024_08_11_03_48_48 Train loss: 4.745517803423856e-18 at step: 426000 lr 7.748409780000004e-05
2024_08_11_03_49_30 Train loss: 2.5238426903939626e-09 at step: 426400 lr 7.748409780000004e-05
2024_08_11_03_50_12 Train loss: 7.454880354522408e-13 at step: 426800 lr 7.748409780000004e-05
2024_08_11_03_50_54 Train loss: 1.2440984348232708e-16 at step: 427200 lr 7.748409780000004e-05
(Val @ epoch 94) acc: 0.99875; ap: 0.9999956783743307
*************************
2024_08_11_03_52_02
(0 progan    ) acc: 100.0; ap: 100.0
(1 stylegan  ) acc: 93.2; ap: 99.0
(2 stylegan2 ) acc: 94.1; ap: 99.7
(3 biggan    ) acc: 76.9; ap: 89.5
(4 cyclegan  ) acc: 68.2; ap: 98.4
(5 stargan   ) acc: 90.4; ap: 99.7
(6 gaugan    ) acc: 78.3; ap: 88.7
(7 deepfake  ) acc: 67.1; ap: 67.3
(8 Mean      ) acc: 83.5; ap: 92.8
*************************
2024_08_11_03_56_11
2024_08_11_03_56_16 Train loss: 2.0300781500282028e-08 at step: 427600 lr 7.748409780000004e-05
2024_08_11_03_56_58 Train loss: 1.1935831025766674e-05 at step: 428000 lr 7.748409780000004e-05
2024_08_11_03_57_39 Train loss: 1.0106970371737134e-08 at step: 428400 lr 7.748409780000004e-05
2024_08_11_03_58_21 Train loss: 6.656319495590424e-09 at step: 428800 lr 7.748409780000004e-05
2024_08_11_03_59_02 Train loss: 1.8647618560407864e-07 at step: 429200 lr 7.748409780000004e-05
2024_08_11_03_59_44 Train loss: 5.307518424757518e-09 at step: 429600 lr 7.748409780000004e-05
2024_08_11_04_00_26 Train loss: 3.255799449486396e-11 at step: 430000 lr 7.748409780000004e-05
2024_08_11_04_01_07 Train loss: 2.8742377922164053e-10 at step: 430400 lr 7.748409780000004e-05
2024_08_11_04_01_49 Train loss: 2.562518990403828e-12 at step: 430800 lr 7.748409780000004e-05
2024_08_11_04_02_31 Train loss: 4.805982833921973e-10 at step: 431200 lr 7.748409780000004e-05
2024_08_11_04_03_12 Train loss: 1.5115262557222064e-12 at step: 431600 lr 7.748409780000004e-05
2024_08_11_04_03_54 Train loss: 2.535900092345833e-13 at step: 432000 lr 7.748409780000004e-05
(Val @ epoch 95) acc: 0.997; ap: 0.9999618658190378
*************************
2024_08_11_04_04_30
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 90.1; ap: 98.6
(2 stylegan2 ) acc: 94.7; ap: 99.9
(3 biggan    ) acc: 70.3; ap: 85.3
(4 cyclegan  ) acc: 65.9; ap: 98.5
(5 stargan   ) acc: 76.1; ap: 100.0
(6 gaugan    ) acc: 67.3; ap: 81.1
(7 deepfake  ) acc: 57.8; ap: 60.2
(8 Mean      ) acc: 77.7; ap: 90.4
*************************
2024_08_11_04_08_35
2024_08_11_04_09_11 Train loss: 2.8826328260164735e-13 at step: 432400 lr 7.748409780000004e-05
2024_08_11_04_09_53 Train loss: 3.207952349940091e-12 at step: 432800 lr 7.748409780000004e-05
2024_08_11_04_10_35 Train loss: 2.898356207814423e-11 at step: 433200 lr 7.748409780000004e-05
2024_08_11_04_11_16 Train loss: 3.4256486536321518e-12 at step: 433600 lr 7.748409780000004e-05
2024_08_11_04_11_58 Train loss: 1.1056661605834961e-05 at step: 434000 lr 7.748409780000004e-05
2024_08_11_04_12_39 Train loss: 3.4295635585991982e-15 at step: 434400 lr 7.748409780000004e-05
2024_08_11_04_13_21 Train loss: 2.7704523245832e-13 at step: 434800 lr 7.748409780000004e-05
2024_08_11_04_14_03 Train loss: 1.0999670141745299e-13 at step: 435200 lr 7.748409780000004e-05
2024_08_11_04_14_44 Train loss: 1.6384621659426557e-08 at step: 435600 lr 7.748409780000004e-05
2024_08_11_04_15_26 Train loss: 4.138259100727737e-06 at step: 436000 lr 7.748409780000004e-05
2024_08_11_04_16_08 Train loss: 6.561900135881592e-18 at step: 436400 lr 7.748409780000004e-05
(Val @ epoch 96) acc: 0.9975; ap: 0.999998003336275
*************************
2024_08_11_04_16_55
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 93.7; ap: 99.6
(2 stylegan2 ) acc: 95.3; ap: 99.8
(3 biggan    ) acc: 81.6; ap: 90.9
(4 cyclegan  ) acc: 75.3; ap: 98.8
(5 stargan   ) acc: 97.2; ap: 99.9
(6 gaugan    ) acc: 84.1; ap: 87.8
(7 deepfake  ) acc: 59.5; ap: 62.1
(8 Mean      ) acc: 85.8; ap: 92.4
*************************
2024_08_11_04_21_01
2024_08_11_04_21_29 Train loss: 1.1921122222702252e-07 at step: 436800 lr 7.748409780000004e-05
2024_08_11_04_22_11 Train loss: 2.2934245445132596e-14 at step: 437200 lr 7.748409780000004e-05
2024_08_11_04_22_52 Train loss: 2.9823318925537023e-08 at step: 437600 lr 7.748409780000004e-05
2024_08_11_04_23_34 Train loss: 5.970704108904101e-08 at step: 438000 lr 7.748409780000004e-05
2024_08_11_04_24_15 Train loss: 9.6560643214616e-06 at step: 438400 lr 7.748409780000004e-05
2024_08_11_04_24_57 Train loss: 1.518740866401913e-08 at step: 438800 lr 7.748409780000004e-05
2024_08_11_04_25_39 Train loss: 1.1683633661246917e-15 at step: 439200 lr 7.748409780000004e-05
2024_08_11_04_26_21 Train loss: 1.7161680063160212e-11 at step: 439600 lr 7.748409780000004e-05
2024_08_11_04_27_03 Train loss: 1.0302766669315067e-12 at step: 440000 lr 7.748409780000004e-05
2024_08_11_04_27_44 Train loss: 1.5251072227329132e-07 at step: 440400 lr 7.748409780000004e-05
2024_08_11_04_28_26 Train loss: 4.4712025193405225e-09 at step: 440800 lr 7.748409780000004e-05
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_98_epoch.pth
(Val @ epoch 97) acc: 0.996125; ap: 0.9999980685843994
*************************
2024_08_11_04_29_24
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 91.7; ap: 99.5
(2 stylegan2 ) acc: 95.0; ap: 100.0
(3 biggan    ) acc: 74.1; ap: 90.4
(4 cyclegan  ) acc: 60.1; ap: 98.7
(5 stargan   ) acc: 82.9; ap: 99.8
(6 gaugan    ) acc: 69.1; ap: 87.4
(7 deepfake  ) acc: 57.8; ap: 58.5
(8 Mean      ) acc: 78.8; ap: 91.8
*************************
2024_08_11_04_33_31
2024_08_11_04_33_46 Train loss: 9.101510517472988e-11 at step: 441200 lr 7.748409780000004e-05
2024_08_11_04_34_28 Train loss: 9.006006660913779e-15 at step: 441600 lr 7.748409780000004e-05
2024_08_11_04_35_10 Train loss: 3.909051713435474e-07 at step: 442000 lr 7.748409780000004e-05
2024_08_11_04_35_51 Train loss: 0.0002123790909536183 at step: 442400 lr 7.748409780000004e-05
2024_08_11_04_36_36 Train loss: 4.8818363411919826e-17 at step: 442800 lr 7.748409780000004e-05
2024_08_11_04_37_20 Train loss: 3.123283386230469e-05 at step: 443200 lr 7.748409780000004e-05
2024_08_11_04_38_05 Train loss: 2.3511306660237473e-23 at step: 443600 lr 7.748409780000004e-05
2024_08_11_04_38_50 Train loss: 1.64474386110669e-07 at step: 444000 lr 7.748409780000004e-05
2024_08_11_04_39_32 Train loss: 4.963391475598655e-09 at step: 444400 lr 7.748409780000004e-05
2024_08_11_04_40_14 Train loss: 1.231944146695696e-08 at step: 444800 lr 7.748409780000004e-05
2024_08_11_04_40_55 Train loss: 1.1474832264250974e-17 at step: 445200 lr 7.748409780000004e-05
(Val @ epoch 98) acc: 0.996375; ap: 0.9999380256252043
*************************
2024_08_11_04_42_04
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.0; ap: 99.9
(2 stylegan2 ) acc: 97.8; ap: 100.0
(3 biggan    ) acc: 81.3; ap: 88.8
(4 cyclegan  ) acc: 72.5; ap: 98.3
(5 stargan   ) acc: 92.7; ap: 100.0
(6 gaugan    ) acc: 82.1; ap: 81.9
(7 deepfake  ) acc: 62.0; ap: 66.0
(8 Mean      ) acc: 85.3; ap: 91.9
*************************
2024_08_11_04_46_09
2024_08_11_04_46_15 Train loss: 4.313561419166945e-08 at step: 445600 lr 7.748409780000004e-05
2024_08_11_04_46_57 Train loss: 1.2495507287300228e-13 at step: 446000 lr 7.748409780000004e-05
2024_08_11_04_47_38 Train loss: 5.960464477539063e-08 at step: 446400 lr 7.748409780000004e-05
2024_08_11_04_48_20 Train loss: 2.943922050580028e-10 at step: 446800 lr 7.748409780000004e-05
2024_08_11_04_49_02 Train loss: 2.5485562549221186e-09 at step: 447200 lr 7.748409780000004e-05
2024_08_11_04_49_43 Train loss: 1.3583407731942998e-09 at step: 447600 lr 7.748409780000004e-05
2024_08_11_04_50_25 Train loss: 5.92084331053639e-15 at step: 448000 lr 7.748409780000004e-05
2024_08_11_04_51_07 Train loss: 7.071146196267455e-21 at step: 448400 lr 7.748409780000004e-05
2024_08_11_04_51_49 Train loss: 2.9802322387695312e-08 at step: 448800 lr 7.748409780000004e-05
2024_08_11_04_52_31 Train loss: 1.0639218672725548e-11 at step: 449200 lr 7.748409780000004e-05
2024_08_11_04_53_12 Train loss: 1.6129861389688888e-13 at step: 449600 lr 7.748409780000004e-05
2024_08_11_04_53_55 Train loss: 9.736494943624052e-11 at step: 450000 lr 7.748409780000004e-05
(Val @ epoch 99) acc: 0.998; ap: 0.9999938171410906
*************************
2024_08_11_04_54_33
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 92.8; ap: 99.8
(2 stylegan2 ) acc: 94.8; ap: 100.0
(3 biggan    ) acc: 74.3; ap: 88.8
(4 cyclegan  ) acc: 62.1; ap: 98.6
(5 stargan   ) acc: 80.2; ap: 100.0
(6 gaugan    ) acc: 69.1; ap: 81.7
(7 deepfake  ) acc: 67.8; ap: 73.3
(8 Mean      ) acc: 80.1; ap: 92.8
*************************
2024_08_11_04_58_40
Saving model ./checkpoints/4-class-resnet-100-epochs/model_epoch_last.pth
